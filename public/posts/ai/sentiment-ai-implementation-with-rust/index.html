<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Rust로 감성분석 AI 구현하기 (LSTM, GRU) | J.log</title>
<meta name=keywords content="AI,LSTM,GRU,Rust,Candle,Huggingface,Transformers,PyTorch"><meta name=description content="개요 파이썬은 내가 제일 많이 사용하는 언어 중 하나이다. 또한 파이썬은 PyTorch, Tensorflow, Keras 등 굉장히 편리하고 사용하기 쉬운 라이브러리가 많다. 하지만 대부분의 파이썬 머신러닝 라이브러리의 중심 부분은 파이썬으로 구현되지 않고 C, C++ 등의 저수준 언어로 구현되어있다. 파이썬은 인공신경망 등의 알고리즘을 실행시키기에는 너무나도 느리기 때문이다.
PyTorch는 인공지능 개발에 필수적인 라이브러리라고 할 수 있다. 파이토치에서 제공하는 수백, 혹은 수천 개의 머신러닝을 위한 API는 코드 몇 줄로 모델을 만들거나, 학습시키고 추론하기 위한 다양한 함수들을 제공한다."><meta name=author content="이재희"><link rel=canonical href=https://blog.hee.blue/posts/ai/sentiment-ai-implementation-with-rust/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://blog.hee.blue/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.hee.blue/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.hee.blue/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.hee.blue/apple-touch-icon.png><link rel=mask-icon href=https://blog.hee.blue/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.hee.blue/posts/ai/sentiment-ai-implementation-with-rust/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Rust로 감성분석 AI 구현하기 (LSTM, GRU)"><meta property="og:description" content="개요 파이썬은 내가 제일 많이 사용하는 언어 중 하나이다. 또한 파이썬은 PyTorch, Tensorflow, Keras 등 굉장히 편리하고 사용하기 쉬운 라이브러리가 많다. 하지만 대부분의 파이썬 머신러닝 라이브러리의 중심 부분은 파이썬으로 구현되지 않고 C, C++ 등의 저수준 언어로 구현되어있다. 파이썬은 인공신경망 등의 알고리즘을 실행시키기에는 너무나도 느리기 때문이다.
PyTorch는 인공지능 개발에 필수적인 라이브러리라고 할 수 있다. 파이토치에서 제공하는 수백, 혹은 수천 개의 머신러닝을 위한 API는 코드 몇 줄로 모델을 만들거나, 학습시키고 추론하기 위한 다양한 함수들을 제공한다."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.hee.blue/posts/ai/sentiment-ai-implementation-with-rust/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-13T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Rust로 감성분석 AI 구현하기 (LSTM, GRU)"><meta name=twitter:description content="개요 파이썬은 내가 제일 많이 사용하는 언어 중 하나이다. 또한 파이썬은 PyTorch, Tensorflow, Keras 등 굉장히 편리하고 사용하기 쉬운 라이브러리가 많다. 하지만 대부분의 파이썬 머신러닝 라이브러리의 중심 부분은 파이썬으로 구현되지 않고 C, C++ 등의 저수준 언어로 구현되어있다. 파이썬은 인공신경망 등의 알고리즘을 실행시키기에는 너무나도 느리기 때문이다.
PyTorch는 인공지능 개발에 필수적인 라이브러리라고 할 수 있다. 파이토치에서 제공하는 수백, 혹은 수천 개의 머신러닝을 위한 API는 코드 몇 줄로 모델을 만들거나, 학습시키고 추론하기 위한 다양한 함수들을 제공한다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.hee.blue/posts/"},{"@type":"ListItem","position":2,"name":"Rust로 감성분석 AI 구현하기 (LSTM, GRU)","item":"https://blog.hee.blue/posts/ai/sentiment-ai-implementation-with-rust/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Rust로 감성분석 AI 구현하기 (LSTM, GRU)","name":"Rust로 감성분석 AI 구현하기 (LSTM, GRU)","description":"개요 파이썬은 내가 제일 많이 사용하는 언어 중 하나이다. 또한 파이썬은 PyTorch, Tensorflow, Keras 등 굉장히 편리하고 사용하기 쉬운 라이브러리가 많다. 하지만 대부분의 파이썬 머신러닝 라이브러리의 중심 부분은 파이썬으로 구현되지 않고 C, C++ 등의 저수준 언어로 구현되어있다. 파이썬은 인공신경망 등의 알고리즘을 실행시키기에는 너무나도 느리기 때문이다.\nPyTorch는 인공지능 개발에 필수적인 라이브러리라고 할 수 있다. 파이토치에서 제공하는 수백, 혹은 수천 개의 머신러닝을 위한 API는 코드 몇 줄로 모델을 만들거나, 학습시키고 추론하기 위한 다양한 함수들을 제공한다.","keywords":["AI","LSTM","GRU","Rust","Candle","Huggingface","Transformers","PyTorch"],"articleBody":"개요 파이썬은 내가 제일 많이 사용하는 언어 중 하나이다. 또한 파이썬은 PyTorch, Tensorflow, Keras 등 굉장히 편리하고 사용하기 쉬운 라이브러리가 많다. 하지만 대부분의 파이썬 머신러닝 라이브러리의 중심 부분은 파이썬으로 구현되지 않고 C, C++ 등의 저수준 언어로 구현되어있다. 파이썬은 인공신경망 등의 알고리즘을 실행시키기에는 너무나도 느리기 때문이다.\nPyTorch는 인공지능 개발에 필수적인 라이브러리라고 할 수 있다. 파이토치에서 제공하는 수백, 혹은 수천 개의 머신러닝을 위한 API는 코드 몇 줄로 모델을 만들거나, 학습시키고 추론하기 위한 다양한 함수들을 제공한다. 실제로 구글 트렌드의 관심도 변화를 보면 2022년 6월을 기준으로 파이토치가 텐서플로우를 앞서는 모습을 보인다.\n그런데, 파이토치나 텐서플로우는 지원하는 기능들이 워낙 많다보니 cpu 환경에서 파이토치를 설치하려면 최소 600MB의 데이터를 다운로드 해야 한다. 그래서 나는 모델 학습은 고사양 컴퓨터에서, 모델 추론은 보다 저사양의 컴퓨터에서 하는 방식이 좀 더 효율적이라고 생각했다. 일반적으로 딥러닝 모델은 추론보다 학습에 더 많은 컴퓨팅 리소스를 사용하기 때문이다.\nHuggingFace는 Tokenizers 라이브러리를 러스트로 구현하였고, 작년부터 Candle 프로젝트를 개발하고 있는데, 이 Candle은 러스트로 만든 머신러닝 프레임워크이다. 그리고 함수 이름과 인덱싱 등을 파이토치와 비슷하게 만들어놔서, 기존 파이토치로 구현한 모델을 러스트로 다시 구현하기에 어려움이 없도록 설계했다는 점이 가장 큰 장점인 것 같다. 파이토치와 비교했을 때 매우 가볍고, CPU 환경에서 작은 사이즈의 단일 실행 파일로 컴파일되어 상당히 마음에 들었다. 그래서 Candle을 사용하여 네이버 영화 리뷰(NSMC) 분류 모델을 만들어 보려고 한다.\n학습 환경 Ubuntu 22.04 Python 3.10.14 PyTorch 2.2.0+cu121 Pandas Tokenizers Numpy tqdm safetensors 추론 환경 Windows 11 Rust 1.78.0 candle 0.5.1 tokenizers 0.19.1 anyhow 1.0.83 데이터셋 준비 네이버 영화 리뷰 데이터셋(NSMC, Naver Sentiment Movie Corpus)은 여기에서 다운로드 받을 수 있다. 필요한 파일은 ratings_train.txt이다.\n모델 구현 및 학습 Python 토크나이저 학습 제일 먼저 토크나이저를 학습시켜야 한다.\nfrom tokenizers import ByteLevelBPETokenizer 나는 한글 데이터를 학습시킬 것이므로 BPE 토크나이저가 아무래도 더 적합하다고 생각했다.\ntokenizer = ByteLevelBPETokenizer(vocab=10000) tokenizer.train(['ratings_train.txt']) 토크나이저의 vocab 크기를 1만개로로 정의하고 “데이터셋 준비” 에서 다운받은 ratings_train.txt 파일을 인자로 넘기면 간단하게 학습이 가능하다. 학습하는 데 대략 30초 정도 소요된다.\ntokenizer.encode(\"재미있는 영화군요\").ids 잘 학습이 됐는지 테스트하기 위해 아무 문장이나 인코딩 해본다.\n출력은 다음과 같다.\n[7457, 20609] tokenizer.save(\"tokenizer.json\") 학습이 완료된 토크나이저를 JSON 형식으로 저장한다.\n모델링 import torch import torch.nn as nn 다음으로 모델링에 필요한 라이브러리를 불러온다. 나는 LSTM, GRU와 같은 인공신경망 알고리즘이 필요하기 때문에 torch.nn 패키지를 nn만 쳐도 쉽게 사용할 수 있도록 하였다.\nhidden_dim = 128 output_dim = 128 num_labels = 2 class TextClassifier(nn.Module): def __init__(self): super(TextClassifier, self).__init__() self.embedding = nn.Embedding(10000, hidden_dim) self.gru = nn.GRU(hidden_dim, output_dim, batch_first=True) self.ln1 = nn.Linear(outptu_dim, num_labels) def forward(self, x): embedded = self.embedding(x) gru_out, hidden = self.gru(embedded) last_hidden = hidden.squeeze(0) logits = self.ln1(last_hidden) return logits 모델 구현 부분은 간단하다. TextClassifier 클래스가 nn.Module 클래스를 상속하도록 하고, 레이어들을 정의하면 된다. 제일 먼저 embedding 레이어는 아까 학습한 토크나이저에 1만개의 vocab이 존재하므로 입력 부분은 토크나이저와 같이 10000을 인자로 넘겼다. 임베딩을 거쳐서 GRU(혹은 LSTM) 레이어에 값이 전달되고, 최종적으로 1과 0(긍정 혹은 부정)을 출력해야 하기 때문에 Linear 레이어의 출력을 2로 설정했다.\nGRU의 batch_first 부분은 (seq, batch, feature)로 된 데이터를 (batch, seq, feature)처럼 배치 사이즈가 맨 앞으로 오게 하라는 뜻이다.\n그리고 forward 함수에는 __init__에서 정의했던 레이어들에 데이터를 전달하는 코드를 작성한다.\n모델 학습 import torch import pandas as pd import torch.nn as nn from tokenizers import Tokenizer from tqdm.auto import tqdm import numpy as np from torch.utils.data import TensorDataset, DataLoader from safetensors.torch import save_file 필요한 라이브러리들을 불러온다.\ndf = pd.read_csv('ratings_train.txt', sep='\\t') 이전에 다운받은 텍스트 파일은 사실 raw 형식이 아니라 tsv 형식이었다.\ndf.head()를 실행하면 다음과 같이 영화 리뷰 앞부분을 미리 볼 수 있다.\nid document label 9976970 아 더빙.. 진짜 짜증나네요 목소리 0 3819312 흠…포스터보고 초딩영화줄….오버연기조차 가볍지 않구나 1 10265843 너무재밓었다그래서보는것을추천한다 0 9045019 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 0 6483659 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서… 1 tokenizer = Tokenizer.from_file('tokenizer.json') 아까 저장된 tokenizer.json 파일을 불러온다.\ndataset = [tokenizer.encode(x).ids for x, y in zip(list(df['document']), list(df['label'])) if str(x) != 'nan' and str(y) != 'nan'] labels = [y for x, y in zip(list(data['document']), list(data['label'])) if str(x) != 'nan' and str(y) != 'nan'] 귀찮아서 한줄로 짰더니 이 꼬라지가 나버렸다. 대충 데이터셋의 document 부분이랑 label 부분을 가져오면서 둘 다 값이 존재하면 리스트에 추가하는 코드라고 이해하면 된다.\ndataset[0]을 입력하면 [333, 2547, 262, 617, 4518, 480, 4492]와 같이 정수로 인코딩된 문장이 나타나고, labels[0]을 입력하면 1 또는 0이 출력된다.\nmax([len(x) for x in dataset]) 모든 문장의 최대 길이를 출력해보면 117이 나온다. 즉 정수로 인코딩 된 데이터셋은 길이가 117을 초과하지 않는다는 뜻이다.\ndef pad_sequences(sentences, max_len): features = np.zeros((len(sentences), max_len), dtype=int) for i, sentence in enumerate(sentences): if len(sentence) != 0: features[i, :len(sentence)] = np.array(sentence)[:max_len] return features padded = pad_sequences(dataset, 128) 넉넉하게 128로 패딩하였다. 패딩 후 train_dataset의 맨 첫 번째 항목은 아래와 비슷하게 나올 것이다.\narray([ 333, 2547, 262, 617, 4518, 480, 4492, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) train_dataset = TensorDataset(torch.tensor(padded), torch.tensor(labels)) train_loader = DataLoader(train_dataset, batch_size=32) TensorDataset을 사용해 정수 리스트로 된 데이터셋을 파이토치에서 사용하는 tensor 형식으로 변환한다.\n그 다음 병렬 학습을 위해 DataLoader에 배치 사이즈를 인자로 넣어 몇 개의 데이터를 병렬로 처리할 건지 설정한다.\nmodel = TextClassifier() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) num_epochs = 6 model.to('cuda' if torch.cuda.is_available() else 'cpu') “모델링\"에서 구현한 TextClassifier 모델을 생성하고, loss 함수와 optimizer, 반복 학습 횟수(num_epochs)를 정한다. 만약 cuda 지원 그래픽카드가 있다면, model.to를 사용하여 GPU에 모델을 올릴 수 있다.\nfor epoch in range(num_epochs): train_loss = 0 model.train() for i, (x, y) in tqdm(enumerate(train_loader)): x, y = x.to('cuda'), y.to('cuda') logits = model(x) loss = criterion(logits, y) optimizer.zero_grad() loss.backward() optimizer.step() train_loss += loss.item() if i % 50 == 0 and i != 0: tqdm.write(f\"Loss: {train_loss / i}\\r\", end='') print(f\"Epoch Loss: {train_loss / len(train_loader)}\") 드디어 학습을 진행한다. 50스텝, 1에폭마다 loss를 출력하게 구현하였다.\n그러면 다음과 같이 결과가 출력되는데, 마음이 편안해진다.\nEpoch Loss: 0.44622183628793516 Epoch Loss: 0.3024691305053341 Epoch Loss: 0.23970229042306188 Epoch Loss: 0.18055053850287833 Epoch Loss: 0.13453298333506641 Epoch Loss: 0.10856294956053496 마지막으로 학습한 모델을 safetensors 형식으로 내보내면 끝이다.\nsave_file(model.state_dict(), \"nsmc.safetensors\") Rust 이미 학습을 진행했고, 모델 파일까지 얻었으므로 Rust 코드에서는 모델만 구현하면 된다.\ncargo new classification과 같이 프로젝트를 새로 만들고, Cargo.toml 파일의 [dependencies] 항목에 다음과 같이 라이브러리를 추가한다.\n[dependencies] anyhow = \"1.0.83\" candle-core = { git = \"https://github.com/huggingface/candle.git\", version = \"0.5.1\" } candle-nn = { git = \"https://github.com/huggingface/candle.git\", version = \"0.5.1\" } tokenizers = \"0.19.1\" 버전은 달라질 수 있으므로 공식 문서를 참조하여 최신 버전 사용을 권장한다.\nCargo.toml에 다음 코드를 추가한다.\n[features] cuda = [\"candle-core/cuda\", \"candle-nn/cuda\"] 만약 시스템이 CUDA를 지원한다면, feature 플래그를 통해 candle의 cuda를 활성화하는 부분이다.\n프로젝트 내의 main.rs 파일의 내용을 전부 지우고 처음부터 시작하는 것이 편하다.\nuse std::io::Write; use anyhow::Error as E; use anyhow::Result; use candle_core::{DType, Device, Tensor}; use candle_nn as nn; use nn::{Module, VarBuilder, RNN}; use tokenizers::Tokenizer; 필요한 crate를 추가한다.\n#[derive(Clone)] struct TextCLassifier { embedding: nn::Embedding, gru: nn::GRU, ln1: nn::Linear, device: Device, } 파이썬과 같이 TextClassifier 모델 구조를 정의한다.\nimpl TextCLassifier { pub fn new(vs: VarBuilder) -\u003e Result\u003cSelf\u003e { let embedding = nn::embedding(10000, 128, vs.pp(\"embedding\"))?; let gru = nn::gru(128, 128, Default::default(), vs.pp(\"gru\"))?; let ln1 = nn::linear(128, 2, vs.pp(\"ln1\"))?; let device = Device::cuda_if_available(0)?; return Ok(Self { embedding, ln1, gru, device, }); } pub fn forward(self, xs: \u0026Tensor) -\u003e Result\u003cTensor\u003e { let xs = self.embedding.forward(xs)?; let mut gru_states = vec![self.gru.zero_state(1)?]; for x in \u0026xs.squeeze(0)?.to_vec2::\u003cf32\u003e()? { let state = self.gru.step( \u0026Tensor::from_vec(x.clone(), (1, x.len()), \u0026self.device)?, \u0026gru_states.last().unwrap(), )?; gru_states.push(state); } let xs = gru_states.last().unwrap().h(); let xs = self.ln1.forward(\u0026xs)?; Ok(xs) } } 파이썬의 __init__과 같이 new 함수를 정의하고, 파이썬에서 구현한 모델에 맞춰 레이어 등을 똑같이 구현해야 한다.\nforward 함수의 내용은 비슷하다.\n추가로 터미널 내에서 입력 \u0026 추론하려면 다음 코드를 impl TextClassifier 블록 안에 넣으면 된다.\npub fn interaction(\u0026mut self, tokenizer: Tokenizer, device: \u0026Device) -\u003e Result\u003c()\u003e { loop { let mut line = String::new(); print!(\"영화 리뷰를 입력하세요: \"); std::io::stdout().flush()?; std::io::stdin().read_line(\u0026mut line)?; if line == \"q\" || line == \"exit\" || line == \"quit\" { break; } let encoded = tokenizer.encode(line, false).map_err(E::msg)?; let data = Tensor::new(vec![encoded.get_ids()], device)?; let result = self.clone().forward(\u0026data)?; let result = result.argmax(1)?.to_vec1::\u003cu32\u003e()?; if result[0] == 0 { println!(\"부정적인 리뷰\"); } else if result[0] == 1 { println!(\"긍정적인 리뷰\"); } else { println!(\"알 수 없음\"); } } Ok(()) } fn main() -\u003e Result\u003c()\u003e { let device = Device::cuda_if_available(0)?; let vs = unsafe { nn::VarBuilder::from_mmaped_safetensors(\u0026[\"nsmc.safetensors\"], DType::F32, \u0026device)? }; let mut model = TextCLassifier::new(vs)?; let tokenizer = Tokenizer::from_file(\"tokenizer.json\").map_err(anyhow::Error::msg)?; model.interaction(tokenizer, \u0026device)?;\\ Ok(()) } 메인 함수에서 safetensors 포맷으로 저장된 모델 파일과 토크나이저를 가져온 후, 생성한 모델의 interaction 함수를 사용하면 터미널과 상호작용 할 수 있다.\n끝 실제 사용 예시\n영화 리뷰를 입력하세요: 와 진짜 재밌는 영화임미다!! 긍정적인 리뷰 영화 리뷰를 입력하세요: 이딴걸 영화라고 처 만들었냐? 부정적인 리뷰 영화 리뷰를 입력하세요: 감독 나와라 ㅋㅋ 부정적인 리뷰 영화 리뷰를 입력하세요: 이 정도면 괜찮은 영화 아닌가? 긍정적인 리뷰 영화 리뷰를 입력하세요: 감동적이네료 ㅠㅜㅜ 긍정적인 리뷰 영화 리뷰를 입력하세요: 볼 가치도 없는듯 ㅋ 부정적인 리뷰 무려 5MB 정도의 작은 모델인데, 직접 영화 리뷰를 써보니 정확도가 90% 이상 나온다고 느꼈다. 거대 언어 모델에 비하면 리뷰를 긍정 또는 부정으로 분류하는 작업밖에 할 수 없어 초라해 보이지만 속도가 중요한 실제 서비스, 또는 임베디드 분야에서 유용할 것 같다는 생각이 들었다.\n사실 성공할 줄은 몰랐는데, 직접 모델을 구현하며 여러 시행착오를 겪었던 점에서 의미가 있는 것 같다.\n","wordCount":"1521","inLanguage":"en","datePublished":"2024-05-13T00:00:00Z","dateModified":"2024-05-13T00:00:00Z","author":{"@type":"Person","name":"이재희"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.hee.blue/posts/ai/sentiment-ai-implementation-with-rust/"},"publisher":{"@type":"Organization","name":"J.log","logo":{"@type":"ImageObject","url":"https://blog.hee.blue/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.hee.blue/ accesskey=h title="J.log (Alt + H)">J.log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.hee.blue/archives title=Archive><span>Archive</span></a></li><li><a href=https://blog.hee.blue/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.hee.blue/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Rust로 감성분석 AI 구현하기 (LSTM, GRU)</h1><div class=post-meta><span title='2024-05-13 00:00:00 +0000 UTC'>May 13, 2024</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;이재희</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%ea%b0%9c%ec%9a%94 aria-label=개요>개요</a></li><li><a href=#%ed%95%99%ec%8a%b5-%ed%99%98%ea%b2%bd aria-label="학습 환경">학습 환경</a></li><li><a href=#%ec%b6%94%eb%a1%a0-%ed%99%98%ea%b2%bd aria-label="추론 환경">추론 환경</a></li><li><a href=#%eb%8d%b0%ec%9d%b4%ed%84%b0%ec%85%8b-%ec%a4%80%eb%b9%84 aria-label="데이터셋 준비">데이터셋 준비</a></li><li><a href=#%eb%aa%a8%eb%8d%b8-%ea%b5%ac%ed%98%84-%eb%b0%8f-%ed%95%99%ec%8a%b5 aria-label="모델 구현 및 학습">모델 구현 및 학습</a><ul><li><a href=#python aria-label=Python>Python</a><ul><li><a href=#%ed%86%a0%ed%81%ac%eb%82%98%ec%9d%b4%ec%a0%80-%ed%95%99%ec%8a%b5 aria-label="토크나이저 학습">토크나이저 학습</a></li><li><a href=#%eb%aa%a8%eb%8d%b8%eb%a7%81 aria-label=모델링>모델링</a></li><li><a href=#%eb%aa%a8%eb%8d%b8-%ed%95%99%ec%8a%b5 aria-label="모델 학습">모델 학습</a></li></ul></li><li><a href=#rust aria-label=Rust>Rust</a></li></ul></li><li><a href=#%eb%81%9d aria-label=끝>끝</a></li></ul></div></details></div><div class=post-content><h1 id=개요>개요<a hidden class=anchor aria-hidden=true href=#개요>#</a></h1><p>파이썬은 내가 제일 많이 사용하는 언어 중 하나이다. 또한 파이썬은 PyTorch, Tensorflow, Keras 등 굉장히 편리하고 사용하기 쉬운 라이브러리가 많다. 하지만 대부분의 파이썬 머신러닝 라이브러리의 중심 부분은 파이썬으로 구현되지 않고 C, C++ 등의 저수준 언어로 구현되어있다. 파이썬은 인공신경망 등의 알고리즘을 실행시키기에는 너무나도 느리기 때문이다.</p><p>PyTorch는 인공지능 개발에 필수적인 라이브러리라고 할 수 있다. 파이토치에서 제공하는 수백, 혹은 수천 개의 머신러닝을 위한 API는 코드 몇 줄로 모델을 만들거나, 학습시키고 추론하기 위한 다양한 함수들을 제공한다. 실제로 구글 트렌드의 관심도 변화를 보면 2022년 6월을 기준으로 파이토치가 텐서플로우를 앞서는 모습을 보인다.</p><p>그런데, 파이토치나 텐서플로우는 지원하는 기능들이 워낙 많다보니 cpu 환경에서 파이토치를 설치하려면 최소 600MB의 데이터를 다운로드 해야 한다. 그래서 나는 모델 학습은 고사양 컴퓨터에서, 모델 추론은 보다 저사양의 컴퓨터에서 하는 방식이 좀 더 효율적이라고 생각했다. 일반적으로 딥러닝 모델은 추론보다 학습에 더 많은 컴퓨팅 리소스를 사용하기 때문이다.</p><p>HuggingFace는 <a href=https://github.com/huggingface/tokenizers target=_blank>Tokenizers</a> 라이브러리를 러스트로 구현하였고, 작년부터 <a href=https://github.com/huggingface/candle target=_blank>Candle</a> 프로젝트를 개발하고 있는데, 이 Candle은 러스트로 만든 머신러닝 프레임워크이다. 그리고 함수 이름과 인덱싱 등을 파이토치와 비슷하게 만들어놔서, 기존 파이토치로 구현한 모델을 러스트로 다시 구현하기에 어려움이 없도록 설계했다는 점이 가장 큰 장점인 것 같다. 파이토치와 비교했을 때 매우 가볍고, CPU 환경에서 작은 사이즈의 단일 실행 파일로 컴파일되어 상당히 마음에 들었다. 그래서 Candle을 사용하여 네이버 영화 리뷰(<a href=https://github.com/e9t/nsmc target=_blank>NSMC</a>) 분류 모델을 만들어 보려고 한다.</p><h1 id=학습-환경>학습 환경<a hidden class=anchor aria-hidden=true href=#학습-환경>#</a></h1><ul><li>Ubuntu 22.04</li><li>Python 3.10.14</li><li>PyTorch 2.2.0+cu121</li><li>Pandas</li><li>Tokenizers</li><li>Numpy</li><li>tqdm</li><li>safetensors</li></ul><h1 id=추론-환경>추론 환경<a hidden class=anchor aria-hidden=true href=#추론-환경>#</a></h1><ul><li>Windows 11</li><li>Rust 1.78.0<ul><li>candle 0.5.1</li><li>tokenizers 0.19.1</li><li>anyhow 1.0.83</li></ul></li></ul><h1 id=데이터셋-준비>데이터셋 준비<a hidden class=anchor aria-hidden=true href=#데이터셋-준비>#</a></h1><p>네이버 영화 리뷰 데이터셋(NSMC, Naver Sentiment Movie Corpus)은 <a href=https://github.com/e9t/nsmc target=_blank>여기</a>에서 다운로드 받을 수 있다. 필요한 파일은 <code>ratings_train.txt</code>이다.</p><h1 id=모델-구현-및-학습>모델 구현 및 학습<a hidden class=anchor aria-hidden=true href=#모델-구현-및-학습>#</a></h1><h2 id=python>Python<a hidden class=anchor aria-hidden=true href=#python>#</a></h2><h3 id=토크나이저-학습>토크나이저 학습<a hidden class=anchor aria-hidden=true href=#토크나이저-학습>#</a></h3><p>제일 먼저 토크나이저를 학습시켜야 한다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tokenizers <span style=color:#f92672>import</span> ByteLevelBPETokenizer
</span></span></code></pre></div><p>나는 한글 데이터를 학습시킬 것이므로 BPE 토크나이저가 아무래도 더 적합하다고 생각했다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> ByteLevelBPETokenizer(vocab<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>)
</span></span><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>train([<span style=color:#e6db74>&#39;ratings_train.txt&#39;</span>])
</span></span></code></pre></div><p>토크나이저의 vocab 크기를 1만개로로 정의하고 &ldquo;데이터셋 준비&rdquo; 에서 다운받은 <code>ratings_train.txt</code> 파일을 인자로 넘기면 간단하게 학습이 가능하다. 학습하는 데 대략 30초 정도 소요된다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>encode(<span style=color:#e6db74>&#34;재미있는 영화군요&#34;</span>)<span style=color:#f92672>.</span>ids
</span></span></code></pre></div><p>잘 학습이 됐는지 테스트하기 위해 아무 문장이나 인코딩 해본다.<br>출력은 다음과 같다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>[<span style=color:#ae81ff>7457</span>, <span style=color:#ae81ff>20609</span>]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#34;tokenizer.json&#34;</span>)
</span></span></code></pre></div><p>학습이 완료된 토크나이저를 JSON 형식으로 저장한다.</p><h3 id=모델링>모델링<a hidden class=anchor aria-hidden=true href=#모델링>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span></code></pre></div><p>다음으로 모델링에 필요한 라이브러리를 불러온다. 나는 LSTM, GRU와 같은 인공신경망 알고리즘이 필요하기 때문에 torch.nn 패키지를 nn만 쳐도 쉽게 사용할 수 있도록 하였다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>hidden_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>output_dim <span style=color:#f92672>=</span> <span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>num_labels <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TextClassifier</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self):
</span></span><span style=display:flex><span>        super(TextClassifier, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>embedding <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Embedding(<span style=color:#ae81ff>10000</span>, hidden_dim)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>gru <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>GRU(hidden_dim, output_dim, batch_first<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>ln1 <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Linear(outptu_dim, num_labels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        embedded <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>embedding(x)
</span></span><span style=display:flex><span>        gru_out, hidden <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>gru(embedded)
</span></span><span style=display:flex><span>        last_hidden <span style=color:#f92672>=</span> hidden<span style=color:#f92672>.</span>squeeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>ln1(last_hidden)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> logits
</span></span></code></pre></div><p>모델 구현 부분은 간단하다. <code>TextClassifier</code> 클래스가 nn.Module 클래스를 상속하도록 하고, 레이어들을 정의하면 된다. 제일 먼저 <code>embedding</code> 레이어는 아까 학습한 토크나이저에 1만개의 vocab이 존재하므로 입력 부분은 토크나이저와 같이 10000을 인자로 넘겼다. 임베딩을 거쳐서 GRU(혹은 LSTM) 레이어에 값이 전달되고, 최종적으로 1과 0(긍정 혹은 부정)을 출력해야 하기 때문에 Linear 레이어의 출력을 2로 설정했다.<br>GRU의 batch_first 부분은 <code>(seq, batch, feature)</code>로 된 데이터를 <code>(batch, seq, feature)</code>처럼 배치 사이즈가 맨 앞으로 오게 하라는 뜻이다.</p><p>그리고 forward 함수에는 <code>__init__</code>에서 정의했던 레이어들에 데이터를 전달하는 코드를 작성한다.</p><h3 id=모델-학습>모델 학습<a hidden class=anchor aria-hidden=true href=#모델-학습>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch.nn <span style=color:#66d9ef>as</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tokenizers <span style=color:#f92672>import</span> Tokenizer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tqdm.auto <span style=color:#f92672>import</span> tqdm
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> TensorDataset, DataLoader
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> safetensors.torch <span style=color:#f92672>import</span> save_file
</span></span></code></pre></div><p>필요한 라이브러리들을 불러온다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(<span style=color:#e6db74>&#39;ratings_train.txt&#39;</span>, sep<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\t</span><span style=color:#e6db74>&#39;</span>)
</span></span></code></pre></div><p>이전에 다운받은 텍스트 파일은 사실 raw 형식이 아니라 tsv 형식이었다.<br><code>df.head()</code>를 실행하면 다음과 같이 영화 리뷰 앞부분을 미리 볼 수 있다.</p><table><thead><tr><th>id</th><th>document</th><th>label</th></tr></thead><tbody><tr><td>9976970</td><td>아 더빙.. 진짜 짜증나네요 목소리</td><td>0</td></tr><tr><td>3819312</td><td>흠&mldr;포스터보고 초딩영화줄&mldr;.오버연기조차 가볍지 않구나</td><td>1</td></tr><tr><td>10265843</td><td>너무재밓었다그래서보는것을추천한다</td><td>0</td></tr><tr><td>9045019</td><td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td><td>0</td></tr><tr><td>6483659</td><td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서&mldr;</td><td>1</td></tr></tbody></table><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> Tokenizer<span style=color:#f92672>.</span>from_file(<span style=color:#e6db74>&#39;tokenizer.json&#39;</span>)
</span></span></code></pre></div><p>아까 저장된 tokenizer.json 파일을 불러온다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dataset <span style=color:#f92672>=</span> [tokenizer<span style=color:#f92672>.</span>encode(x)<span style=color:#f92672>.</span>ids <span style=color:#66d9ef>for</span> x, y <span style=color:#f92672>in</span> zip(list(df[<span style=color:#e6db74>&#39;document&#39;</span>]), list(df[<span style=color:#e6db74>&#39;label&#39;</span>])) <span style=color:#66d9ef>if</span> str(x) <span style=color:#f92672>!=</span> <span style=color:#e6db74>&#39;nan&#39;</span> <span style=color:#f92672>and</span> str(y) <span style=color:#f92672>!=</span> <span style=color:#e6db74>&#39;nan&#39;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>labels <span style=color:#f92672>=</span> [y <span style=color:#66d9ef>for</span> x, y <span style=color:#f92672>in</span> zip(list(data[<span style=color:#e6db74>&#39;document&#39;</span>]), list(data[<span style=color:#e6db74>&#39;label&#39;</span>])) <span style=color:#66d9ef>if</span> str(x) <span style=color:#f92672>!=</span> <span style=color:#e6db74>&#39;nan&#39;</span> <span style=color:#f92672>and</span> str(y) <span style=color:#f92672>!=</span> <span style=color:#e6db74>&#39;nan&#39;</span>]
</span></span></code></pre></div><p>귀찮아서 한줄로 짰더니 이 꼬라지가 나버렸다. 대충 데이터셋의 document 부분이랑 label 부분을 가져오면서 둘 다 값이 존재하면 리스트에 추가하는 코드라고 이해하면 된다.</p><p><code>dataset[0]</code>을 입력하면 <code>[333, 2547, 262, 617, 4518, 480, 4492]</code>와 같이 정수로 인코딩된 문장이 나타나고, <code>labels[0]</code>을 입력하면 1 또는 0이 출력된다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>max([len(x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> dataset])
</span></span></code></pre></div><p>모든 문장의 최대 길이를 출력해보면 117이 나온다. 즉 정수로 인코딩 된 데이터셋은 길이가 117을 초과하지 않는다는 뜻이다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>pad_sequences</span>(sentences, max_len):
</span></span><span style=display:flex><span>    features <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((len(sentences), max_len), dtype<span style=color:#f92672>=</span>int)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, sentence <span style=color:#f92672>in</span> enumerate(sentences):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(sentence) <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            features[i, :len(sentence)] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(sentence)[:max_len]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> features
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>padded <span style=color:#f92672>=</span> pad_sequences(dataset, <span style=color:#ae81ff>128</span>)
</span></span></code></pre></div><p>넉넉하게 128로 패딩하였다. 패딩 후 <code>train_dataset</code>의 맨 첫 번째 항목은 아래와 비슷하게 나올 것이다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>array([ <span style=color:#ae81ff>333</span>, <span style=color:#ae81ff>2547</span>,  <span style=color:#ae81ff>262</span>,  <span style=color:#ae81ff>617</span>, <span style=color:#ae81ff>4518</span>,  <span style=color:#ae81ff>480</span>, <span style=color:#ae81ff>4492</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>,    <span style=color:#ae81ff>0</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_dataset <span style=color:#f92672>=</span> TensorDataset(torch<span style=color:#f92672>.</span>tensor(padded), torch<span style=color:#f92672>.</span>tensor(labels))
</span></span><span style=display:flex><span>train_loader <span style=color:#f92672>=</span> DataLoader(train_dataset, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>)
</span></span></code></pre></div><p>TensorDataset을 사용해 정수 리스트로 된 데이터셋을 파이토치에서 사용하는 tensor 형식으로 변환한다.<br>그 다음 병렬 학습을 위해 DataLoader에 배치 사이즈를 인자로 넣어 몇 개의 데이터를 병렬로 처리할 건지 설정한다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> TextClassifier()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>criterion <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>CrossEntropyLoss()
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Adam(model<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.001</span>)
</span></span><span style=display:flex><span>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>6</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#39;cuda&#39;</span> <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available() <span style=color:#66d9ef>else</span> <span style=color:#e6db74>&#39;cpu&#39;</span>)
</span></span></code></pre></div><p>&ldquo;모델링"에서 구현한 <code>TextClassifier</code> 모델을 생성하고, loss 함수와 optimizer, 반복 학습 횟수(num_epochs)를 정한다. 만약 cuda 지원 그래픽카드가 있다면, <code>model.to</code>를 사용하여 GPU에 모델을 올릴 수 있다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>    train_loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, (x, y) <span style=color:#f92672>in</span> tqdm(enumerate(train_loader)):
</span></span><span style=display:flex><span>        x, y <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#39;cuda&#39;</span>), y<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#39;cuda&#39;</span>)
</span></span><span style=display:flex><span>        logits <span style=color:#f92672>=</span> model(x)
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> criterion(logits, y)
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        train_loss <span style=color:#f92672>+=</span> loss<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> i <span style=color:#f92672>%</span> <span style=color:#ae81ff>50</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> i <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            tqdm<span style=color:#f92672>.</span>write(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Loss: </span><span style=color:#e6db74>{</span>train_loss <span style=color:#f92672>/</span> i<span style=color:#e6db74>}</span><span style=color:#ae81ff>\r</span><span style=color:#e6db74>&#34;</span>, end<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch Loss: </span><span style=color:#e6db74>{</span>train_loss <span style=color:#f92672>/</span> len(train_loader)<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p>드디어 학습을 진행한다. 50스텝, 1에폭마다 loss를 출력하게 구현하였다.<br>그러면 다음과 같이 결과가 출력되는데, 마음이 편안해진다.</p><pre tabindex=0><code>Epoch Loss: 0.44622183628793516
Epoch Loss: 0.3024691305053341
Epoch Loss: 0.23970229042306188
Epoch Loss: 0.18055053850287833
Epoch Loss: 0.13453298333506641
Epoch Loss: 0.10856294956053496
</code></pre><p>마지막으로 학습한 모델을 safetensors 형식으로 내보내면 끝이다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>save_file(model<span style=color:#f92672>.</span>state_dict(), <span style=color:#e6db74>&#34;nsmc.safetensors&#34;</span>)
</span></span></code></pre></div><h2 id=rust>Rust<a hidden class=anchor aria-hidden=true href=#rust>#</a></h2><p>이미 학습을 진행했고, 모델 파일까지 얻었으므로 Rust 코드에서는 모델만 구현하면 된다.<br><code>cargo new classification</code>과 같이 프로젝트를 새로 만들고, <code>Cargo.toml</code> 파일의 <code>[dependencies]</code> 항목에 다음과 같이 라이브러리를 추가한다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[<span style=color:#a6e22e>dependencies</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>anyhow</span> = <span style=color:#e6db74>&#34;1.0.83&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>candle-core</span> = { <span style=color:#a6e22e>git</span> = <span style=color:#e6db74>&#34;https://github.com/huggingface/candle.git&#34;</span>, <span style=color:#a6e22e>version</span> = <span style=color:#e6db74>&#34;0.5.1&#34;</span> }
</span></span><span style=display:flex><span><span style=color:#a6e22e>candle-nn</span> = { <span style=color:#a6e22e>git</span> = <span style=color:#e6db74>&#34;https://github.com/huggingface/candle.git&#34;</span>, <span style=color:#a6e22e>version</span> = <span style=color:#e6db74>&#34;0.5.1&#34;</span> }
</span></span><span style=display:flex><span><span style=color:#a6e22e>tokenizers</span> = <span style=color:#e6db74>&#34;0.19.1&#34;</span>
</span></span></code></pre></div><p>버전은 달라질 수 있으므로 공식 문서를 참조하여 최신 버전 사용을 권장한다.</p><p><code>Cargo.toml</code>에 다음 코드를 추가한다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[<span style=color:#a6e22e>features</span>]
</span></span><span style=display:flex><span><span style=color:#a6e22e>cuda</span> = [<span style=color:#e6db74>&#34;candle-core/cuda&#34;</span>, <span style=color:#e6db74>&#34;candle-nn/cuda&#34;</span>]
</span></span></code></pre></div><p>만약 시스템이 CUDA를 지원한다면, feature 플래그를 통해 candle의 cuda를 활성화하는 부분이다.</p><p>프로젝트 내의 <code>main.rs</code> 파일의 내용을 전부 지우고 처음부터 시작하는 것이 편하다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>use</span> std::io::Write;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> anyhow::Error <span style=color:#66d9ef>as</span> E;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> anyhow::Result;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> candle_core::{DType, Device, Tensor};
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> candle_nn <span style=color:#66d9ef>as</span> nn;
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> nn::{Module, VarBuilder, <span style=color:#66d9ef>RNN</span>};
</span></span><span style=display:flex><span><span style=color:#66d9ef>use</span> tokenizers::Tokenizer;
</span></span></code></pre></div><p>필요한 crate를 추가한다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#75715e>#[derive(Clone)]</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>struct</span> <span style=color:#a6e22e>TextCLassifier</span> {
</span></span><span style=display:flex><span>    embedding: <span style=color:#a6e22e>nn</span>::Embedding,
</span></span><span style=display:flex><span>    gru: <span style=color:#a6e22e>nn</span>::<span style=color:#66d9ef>GRU</span>,
</span></span><span style=display:flex><span>    ln1: <span style=color:#a6e22e>nn</span>::Linear,
</span></span><span style=display:flex><span>    device: <span style=color:#a6e22e>Device</span>,
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>파이썬과 같이 TextClassifier 모델 구조를 정의한다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>impl</span> TextCLassifier {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>new</span>(vs: <span style=color:#a6e22e>VarBuilder</span>) -&gt; Result<span style=color:#f92672>&lt;</span>Self<span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> embedding <span style=color:#f92672>=</span> nn::embedding(<span style=color:#ae81ff>10000</span>, <span style=color:#ae81ff>128</span>, vs.pp(<span style=color:#e6db74>&#34;embedding&#34;</span>))<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> gru <span style=color:#f92672>=</span> nn::gru(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>128</span>, Default::default(), vs.pp(<span style=color:#e6db74>&#34;gru&#34;</span>))<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> ln1 <span style=color:#f92672>=</span> nn::linear(<span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>2</span>, vs.pp(<span style=color:#e6db74>&#34;ln1&#34;</span>))<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> device <span style=color:#f92672>=</span> Device::cuda_if_available(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> Ok(Self {
</span></span><span style=display:flex><span>            embedding,
</span></span><span style=display:flex><span>            ln1,
</span></span><span style=display:flex><span>            gru,
</span></span><span style=display:flex><span>            device,
</span></span><span style=display:flex><span>        });
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>forward</span>(self, xs: <span style=color:#66d9ef>&amp;</span><span style=color:#a6e22e>Tensor</span>) -&gt; Result<span style=color:#f92672>&lt;</span>Tensor<span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> xs <span style=color:#f92672>=</span> self.embedding.forward(xs)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> gru_states <span style=color:#f92672>=</span> vec![self.gru.zero_state(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>?</span>];
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> x <span style=color:#66d9ef>in</span> <span style=color:#f92672>&amp;</span>xs.squeeze(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>?</span>.to_vec2::<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>f32</span><span style=color:#f92672>&gt;</span>()<span style=color:#f92672>?</span> {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> state <span style=color:#f92672>=</span> self.gru.step(
</span></span><span style=display:flex><span>                <span style=color:#f92672>&amp;</span>Tensor::from_vec(x.clone(), (<span style=color:#ae81ff>1</span>, x.len()), <span style=color:#f92672>&amp;</span>self.device)<span style=color:#f92672>?</span>,
</span></span><span style=display:flex><span>                <span style=color:#f92672>&amp;</span>gru_states.last().unwrap(),
</span></span><span style=display:flex><span>            )<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>            gru_states.push(state);
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> xs <span style=color:#f92672>=</span> gru_states.last().unwrap().h();
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> xs <span style=color:#f92672>=</span> self.ln1.forward(<span style=color:#f92672>&amp;</span>xs)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>        Ok(xs)
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>파이썬의 <code>__init__</code>과 같이 <code>new</code> 함수를 정의하고, 파이썬에서 구현한 모델에 맞춰 레이어 등을 똑같이 구현해야 한다.<br><code>forward</code> 함수의 내용은 비슷하다.<br>추가로 터미널 내에서 입력 & 추론하려면 다음 코드를 <code>impl TextClassifier</code> 블록 안에 넣으면 된다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>interaction</span>(<span style=color:#f92672>&amp;</span><span style=color:#66d9ef>mut</span> self, tokenizer: <span style=color:#a6e22e>Tokenizer</span>, device: <span style=color:#66d9ef>&amp;</span><span style=color:#a6e22e>Device</span>) -&gt; Result<span style=color:#f92672>&lt;</span>()<span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>loop</span> {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> line <span style=color:#f92672>=</span> String::new();
</span></span><span style=display:flex><span>		print!(<span style=color:#e6db74>&#34;영화 리뷰를 입력하세요: &#34;</span>);
</span></span><span style=display:flex><span>		std::io::stdout().flush()<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>		std::io::stdin().read_line(<span style=color:#f92672>&amp;</span><span style=color:#66d9ef>mut</span> line)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> line <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;q&#34;</span> <span style=color:#f92672>||</span> line <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;exit&#34;</span> <span style=color:#f92672>||</span> line <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;quit&#34;</span> {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>break</span>;
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>let</span> encoded <span style=color:#f92672>=</span> tokenizer.encode(line, <span style=color:#66d9ef>false</span>).map_err(E::msg)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>let</span> data <span style=color:#f92672>=</span> Tensor::new(vec![encoded.get_ids()], device)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> self.clone().forward(<span style=color:#f92672>&amp;</span>data)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>let</span> result <span style=color:#f92672>=</span> result.argmax(<span style=color:#ae81ff>1</span>)<span style=color:#f92672>?</span>.to_vec1::<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>u32</span><span style=color:#f92672>&gt;</span>()<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> result[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span> {
</span></span><span style=display:flex><span>			println!(<span style=color:#e6db74>&#34;부정적인 리뷰&#34;</span>);
</span></span><span style=display:flex><span>		} <span style=color:#66d9ef>else</span> <span style=color:#66d9ef>if</span> result[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span> {
</span></span><span style=display:flex><span>			println!(<span style=color:#e6db74>&#34;긍정적인 리뷰&#34;</span>);
</span></span><span style=display:flex><span>		} <span style=color:#66d9ef>else</span> {
</span></span><span style=display:flex><span>			println!(<span style=color:#e6db74>&#34;알 수 없음&#34;</span>);
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	Ok(())
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>main</span>() -&gt; Result<span style=color:#f92672>&lt;</span>()<span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> device <span style=color:#f92672>=</span> Device::cuda_if_available(<span style=color:#ae81ff>0</span>)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> vs <span style=color:#f92672>=</span> <span style=color:#66d9ef>unsafe</span> {
</span></span><span style=display:flex><span>        nn::VarBuilder::from_mmaped_safetensors(<span style=color:#f92672>&amp;</span>[<span style=color:#e6db74>&#34;nsmc.safetensors&#34;</span>], DType::F32, <span style=color:#f92672>&amp;</span>device)<span style=color:#f92672>?</span>
</span></span><span style=display:flex><span>    };
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> model <span style=color:#f92672>=</span> TextCLassifier::new(vs)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>let</span> tokenizer <span style=color:#f92672>=</span> Tokenizer::from_file(<span style=color:#e6db74>&#34;tokenizer.json&#34;</span>).map_err(anyhow::Error::msg)<span style=color:#f92672>?</span>;
</span></span><span style=display:flex><span>    model.interaction(tokenizer, <span style=color:#f92672>&amp;</span>device)<span style=color:#f92672>?</span>;<span style=color:#960050;background-color:#1e0010>\</span>
</span></span><span style=display:flex><span>    Ok(())
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>메인 함수에서 safetensors 포맷으로 저장된 모델 파일과 토크나이저를 가져온 후, 생성한 모델의 interaction 함수를 사용하면 터미널과 상호작용 할 수 있다.</p><h1 id=끝>끝<a hidden class=anchor aria-hidden=true href=#끝>#</a></h1><p><strong>실제 사용 예시</strong></p><pre tabindex=0><code class=language-raw data-lang=raw>영화 리뷰를 입력하세요: 와 진짜 재밌는 영화임미다!!
긍정적인 리뷰
영화 리뷰를 입력하세요: 이딴걸 영화라고 처 만들었냐?
부정적인 리뷰
영화 리뷰를 입력하세요: 감독 나와라 ㅋㅋ
부정적인 리뷰
영화 리뷰를 입력하세요: 이 정도면 괜찮은 영화 아닌가?
긍정적인 리뷰
영화 리뷰를 입력하세요: 감동적이네료 ㅠㅜㅜ
긍정적인 리뷰
영화 리뷰를 입력하세요: 볼 가치도 없는듯 ㅋ
부정적인 리뷰
</code></pre><p>무려 5MB 정도의 작은 모델인데, 직접 영화 리뷰를 써보니 정확도가 90% 이상 나온다고 느꼈다. 거대 언어 모델에 비하면 리뷰를 긍정 또는 부정으로 분류하는 작업밖에 할 수 없어 초라해 보이지만 속도가 중요한 실제 서비스, 또는 임베디드 분야에서 유용할 것 같다는 생각이 들었다.<br>사실 성공할 줄은 몰랐는데, 직접 모델을 구현하며 여러 시행착오를 겪었던 점에서 의미가 있는 것 같다.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.hee.blue/tags/ai/>AI</a></li><li><a href=https://blog.hee.blue/tags/lstm/>LSTM</a></li><li><a href=https://blog.hee.blue/tags/gru/>GRU</a></li><li><a href=https://blog.hee.blue/tags/rust/>Rust</a></li><li><a href=https://blog.hee.blue/tags/candle/>Candle</a></li><li><a href=https://blog.hee.blue/tags/huggingface/>Huggingface</a></li><li><a href=https://blog.hee.blue/tags/transformers/>Transformers</a></li><li><a href=https://blog.hee.blue/tags/pytorch/>PyTorch</a></li></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>