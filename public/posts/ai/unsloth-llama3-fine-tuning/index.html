<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Efficient LLaMA3 Fine-Tuning with Unsloth | J.log</title>
<meta name=keywords content="LLaMA,LLM,AI,Fine-Tuning,Unsloth,Conversational"><meta name=description content="개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.
학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.1 충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러) 초연한 자세 어떠한 오류가 터져도 화내지 않는 강건한 정신 채팅 데이터셋 AI Hub에서 가져온 주제별 텍스트 일상 대화 데이터 데이터 전처리 라이브러리(pypi) unsloth accelerate transformers trl datasets peft etc."><meta name=author content="이재희"><link rel=canonical href=https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://blog.hee.blue/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.hee.blue/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.hee.blue/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.hee.blue/apple-touch-icon.png><link rel=mask-icon href=https://blog.hee.blue/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Efficient LLaMA3 Fine-Tuning with Unsloth"><meta property="og:description" content="개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.
학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.1 충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러) 초연한 자세 어떠한 오류가 터져도 화내지 않는 강건한 정신 채팅 데이터셋 AI Hub에서 가져온 주제별 텍스트 일상 대화 데이터 데이터 전처리 라이브러리(pypi) unsloth accelerate transformers trl datasets peft etc."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-10T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-10T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Efficient LLaMA3 Fine-Tuning with Unsloth"><meta name=twitter:description content="개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.
학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.1 충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러) 초연한 자세 어떠한 오류가 터져도 화내지 않는 강건한 정신 채팅 데이터셋 AI Hub에서 가져온 주제별 텍스트 일상 대화 데이터 데이터 전처리 라이브러리(pypi) unsloth accelerate transformers trl datasets peft etc."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.hee.blue/posts/"},{"@type":"ListItem","position":2,"name":"Efficient LLaMA3 Fine-Tuning with Unsloth","item":"https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Efficient LLaMA3 Fine-Tuning with Unsloth","name":"Efficient LLaMA3 Fine-Tuning with Unsloth","description":"개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.\n학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.1 충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러) 초연한 자세 어떠한 오류가 터져도 화내지 않는 강건한 정신 채팅 데이터셋 AI Hub에서 가져온 주제별 텍스트 일상 대화 데이터 데이터 전처리 라이브러리(pypi) unsloth accelerate transformers trl datasets peft etc.","keywords":["LLaMA","LLM","AI","Fine-Tuning","Unsloth","Conversational"],"articleBody":"개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.\n학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.1 충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러) 초연한 자세 어떠한 오류가 터져도 화내지 않는 강건한 정신 채팅 데이터셋 AI Hub에서 가져온 주제별 텍스트 일상 대화 데이터 데이터 전처리 라이브러리(pypi) unsloth accelerate transformers trl datasets peft etc. 채팅 데이터셋 전처리 JSON으로 구성된 채팅 데이터셋은 대충 전처리하면 다음과 같은 텍스트 형태가 된다.\nuser: 점심 메뉴 정하신 분 bot: 누룽지 끓여 먹을까 고민 user: 나 아침 든든하게 먹으니 힘 난다 user: 아 나 점심 뭐 먹지 bot: 점심은 사모님이 주심 후후 user: 나 점심 내장국밥 픽 user: **가 오늘 만들어야 함? bot: 누룽지는 간식으로 먹어야징 user: 돼지국밥으로 바꿀까 user: 누룽지 맛있겠어... 나두 bot: 여기 먹을 거 개많음 ㄷㄷ user: 아니면 두루치기 먹을가 user: 국밥은 언제나 맛있음 bot: 떡볶이 이런 거 먹을 수 있음 키키 user: 먹는 게 제이 좋아! user: 국밥 1그릇 배달 안 해주겠지 ㅠ bot: 수제비랑 밀키트 다 나왕 user: 떡볶이 파는 데가 없음 여기 \u003c|endoftext|\u003e 여기서 문제가 하나 발생한다. 우리는 로봇과 1대1로 대화하기를 원하는데, 이 데이터셋은 화자가 최대 3명이기 때문에 1번 화자는 user, 2번 화자는 bot, 3번 화자는 다시 user로 치환해버려서 user는 이중인격자가 되는 것을 확인할 수 있다. 이 부분은 학습이 완료된 후 알아채서 다음에는 user1, user2, user3, ...이런 식으로 시도해보려고 한다.\n또한 이름도 개인정보 보호 차원에서 별 모양 기호(asterisk, *)로 뜨는데, 적당하게 으로 바꿨으면 더 좋았을 것 같다.\n마지막으로 오타가 상당히 많은데, 오타 좀 내면 더욱 사람과 비슷해지지 않을까 하는 생각이 들어 오타 교정은 수행하지 않았다. 사실 몇만 줄의 텍스트 파일을 맞춤법 검사기를 돌려 일일이 수정하는 것도 미친 짓이라고 생각했다.\n학습 코드 import torch print(torch.__version__) from tqdm.auto import tqdm from unsloth import FastLanguageModel from trl import SFTTrainer from datasets import load_dataset from transformers import TrainingArguments 먼저 필요한 모듈을 import한다. 여기서 unsloth 모듈은 뭐하는 놈일까?\nUnsloth는 무엇인가 unsloth는 여러 언어모델들을 파인튜닝 시 2배~5배 빠르게, 메모리 사용량을 80%까지 줄일 수 있다고 주장하는 라이브러리다. 실제로 vram 사용량을 꽤 줄였던 것으로 기억한다.\n1 A100 40GB 🤗Hugging Face Flash Attention 🦥Unsloth Open Source 🦥Unsloth Pro Alpaca 1x 1.04x 1.98x 15.64x LAION Chip2 1x 0.92x 1.61x 20.73x OASST 1x 1.19x 2.17x 14.83x Slim Orca 1x 1.18x 2.22x 14.82x 위 표는 unsloth의 깃허브 readme에서 가져온 벤치마크 결과이다. 최대 2배 빠르다는 건 이해할 수 있지만, 아무리 봐도 14~15배 빨라진다는 unsloth pro는 그짓말 같긴 하다.\n아무튼 unsloth는 파이토치 버전, 쿠다 버전, GPU Compute Capability에 따라 설치 명령어가 약간씩 다르니 꼭 README를 참고하여 설치하자.\n모델 \u0026 토크나이저 다운로드 model, tokenizer = FastLanguageModel.from_pretrained('beomi/Llama-3-Open-Ko-8B', max_seq_length=1024, dtype=None, load_in_4bit=True) unsloth에서 제공하는 FastLanguageModel을 사용하여 transformers 라이브러리와 비슷한 방식으로 모델을 다운로드 받을 수 있다.\n그리고 load_in_4bit 인자를 사용하여 모델을 4비트 양자화한 상태로 불러온다. 양자화를 하지 않으면 나중에 학습할 때 VRAM 점유율이 4090의 24GB를 초과하여 OOM(CUDA Out Of Memory) 오류가 터진다. 다만 양자화로 인해 모델의 성능 저하도 있는 것 같다. max_seq_length는 입력 토큰의 최대 길이이다. 1024 토큰이 좀 작을 수도 있지만, OOM이 무서워서 일단 1024로 했다. 나중에 RoPE Scaling이라는 기술을 활용하면 모델의 context size를 파인튜닝 없이 늘릴 수 있다고 한다.\nPEFT 모델로 변환 model = FastLanguageModel.get_peft_model( model, r=16, target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",], lora_alpha=32, lora_dropout=0, bias=\"none\", use_gradient_checkpointing=\"unsloth\", max_seq_length=1024, use_rslora=False, loftq_config=None, ) PEFT(Parameter Efficient Fine-Tuning)는 Huggingface 커뮤니티에서 개발 중인 라이브러리다. PEFT는 모델의 전체 파라미터를 학습하는 대신, LoRA와 같은 기술을 사용하여 필요한 작은 수의 파라미터만 학습해서 컴퓨팅 비용을 줄일 수 있는 모듈이다. 기존 학습 코드를 조금만 수정하면 효율적으로 학습이 가능하다는 점이 장점이다.\nUnsloth는 PEFT 모듈 + $\\alpha$를 활용해서 속도와 메모리 사용량을 줄이는 방식이기 때문에 두 라이브러리는 다르다.\nUnsloth에서 제공하는 FastLanguageModel에서 모델을 PEFT 모델로 변환할 수 있다. 내부적으로 LoRA를 사용하는 듯하다. LoRA 기술에 대해서는 기본적인 동작 원리만 이해하고 자세히는 알지 못해서 나중에 따로 공부해보고 싶다.\n데이터셋 불러오기 dataset = load_dataset('json', data_files='chat.json', split='train') huggingface datasets 모듈으로 아까 전처리한 텍스트 파일을 대화 주제 단위로 자르고, 다음과 같은 JSON 형식으로 변환해서 load_dataset 함수로 불러왔다.\n[ { \"text\": \"user: 점심 메뉴 정하신 분 bot: 누룽지 끓여 먹을까 고민...[이하생략]\u003e\" } ] Trainer 구현 trainer = SFTTrainer( model=model, train_dataset=dataset, dataset_text_field='text', max_seq_length=1024, tokenizer=tokenizer, args=TrainingArguments( per_device_train_batch_size=2, gradient_accumulation_steps=4, warmup_steps=10, bf16=True, logging_steps=10, output_dir='results', optim='adamw_8bit', ), ) trl에서 제공하는 SFTTrainer(Supervised Fine-Tuning Trainer)를 사용하면 쉽게 파인튜닝할 수 있다. 여기서 중요한 코드가 TrainingArguments 부분이라고 생각하는데, 학습에 필요한 learning rate 등을 TrainingArguments에 인자로 넘기는 부분이기 때문이다. per_device_train_batch_size 대신 auto_find_batch_size=True를 쓰면 자동으로 최적의 batch size를 찾아준다. 사실 TrainingArguments는 unsloth의 readme를 귀찮아서 대충 그냥 갖다 쓴거라 뭔지는 나도 잘 모른다.\n학습 이제 학습은 굉장히 간단해진다.\ntrainer.train() 끝.\nUnsloth가 편리한게 다음과 같이 코드를 입력하면 자동으로 llama.cpp에서 돌아가는 GGUF 파일 형식으로 변환하고 양자화까지 해준다는 점이다.\nmodel.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q4_k_m\") model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"q8_0\") model.save_pretrained_gguf(\"dir\", tokenizer, quantization_method = \"f16\") 학습 결과 이제 챗봇과 대화를 해보자.\n대화 기록\nuser: 안녕 bot: 안녕 user: 오늘 날씨 좋다 ㅎㅎ bot: 오늘 날씨가 너무 좋아 키키 user: 밖에 나가자 bot: 그러게 완전 가을이야~ user: 여름인데? bot: 여름이 아니지 않나? user: 여름맞는데? bot: 아직 여름인 거 같은데? user: 지금 내 말이 틀렸다고 주장하는 거니? bot: 그렇다고 할 수는 없잖아~ 나도 좋아 키키 내가 파인튜닝을 너무 대충 해서 그런지는 모르겠는데, 대화를 하면 할 수록 배가 산으로 가는 현상을 볼 수 있다.\n아니면 약 4MB 용량의 적은 데이터셋으로 학습을 진행해서 성능이 구린 이유도 있는 것 같다.\n마치며 대략 2년 전부터 진짜 사람같은 챗봇을 만들고 싶어서 여러가지 시도를 해보고 있다. ChatGPT 등 성능이 좋은 모델의 api를 사용하면 되지 않느냐고 생각할 수 있지만, API를 사용하는 방식은 내 성미에 맞지 않기도 하고 gpt 특유의 어색한 말투가 너무 부자연스러워서 대화에 몰입할 수 없었다.\n재미있는 건 1년 전에 EleutherAI의 polyglot-ko라는 pretrained 모델을 사용해서 파인튜닝한 결과물이 지금보다 좋았다고 느꼈다는 사실이다. 아무래도 polyglot-ko는 완전히 처음부터 한국어 데이터로 pretraining한 반면, 라마와 같은 모델은 영어의 비중이 높고, 한국어가 거의 없는 데이터셋으로 학습 후 누군가 한국어로 다시 파인튜닝했기 때문에 한국어 능력이 부족하지 않았나 추측해본다.\n라마와 같은 pretrained 모델 하나를 만드려면 엄청난 비용과 시간이 든다. 그럼에도 불구하고 2022년에 공개된 polyglot-ko 이후 처음부터 한국어로 학습한 모델이 전무하다고 할 수 있을 정도로 현재 한국어 LLM 생태계는 기존 영어 모델을 파인튜닝하고, 다시 파인튜닝하는 부분에 초점이 맞춰져있는 것 같다. 그래서 나는 네이버 등의 회사가 비교적 작은 모델인 한국어 sLLM을 공개했으면 좋겠다는 바람이 있다.\n개인적인 의견이므로 부정확할 수 있으니 참고하지 마시오.\n","wordCount":"1017","inLanguage":"en","datePublished":"2024-05-10T00:00:00Z","dateModified":"2024-05-10T00:00:00Z","author":{"@type":"Person","name":"이재희"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/"},"publisher":{"@type":"Organization","name":"J.log","logo":{"@type":"ImageObject","url":"https://blog.hee.blue/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.hee.blue/ accesskey=h title="J.log (Alt + H)">J.log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://blog.hee.blue/archives title=Archive><span>Archive</span></a></li><li><a href=https://blog.hee.blue/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://blog.hee.blue/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Efficient LLaMA3 Fine-Tuning with Unsloth</h1><div class=post-meta><span title='2024-05-10 00:00:00 +0000 UTC'>May 10, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;이재희</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%ea%b0%9c%ec%9a%94 aria-label=개요>개요</a></li><li><a href=#%ed%95%99%ec%8a%b5-%ec%a0%84-%ec%a4%80%eb%b9%84%eb%ac%bc aria-label="학습 전 준비물">학습 전 준비물</a></li><li><a href=#%ec%b1%84%ed%8c%85-%eb%8d%b0%ec%9d%b4%ed%84%b0%ec%85%8b-%ec%a0%84%ec%b2%98%eb%a6%ac aria-label="채팅 데이터셋 전처리">채팅 데이터셋 전처리</a></li><li><a href=#%ed%95%99%ec%8a%b5-%ec%bd%94%eb%93%9c aria-label="학습 코드">학습 코드</a><ul><li><a href=#unsloth%eb%8a%94-%eb%ac%b4%ec%97%87%ec%9d%b8%ea%b0%80 aria-label="Unsloth는 무엇인가">Unsloth는 무엇인가</a></li><li><a href=#%eb%aa%a8%eb%8d%b8--%ed%86%a0%ed%81%ac%eb%82%98%ec%9d%b4%ec%a0%80-%eb%8b%a4%ec%9a%b4%eb%a1%9c%eb%93%9c aria-label="모델 & 토크나이저 다운로드">모델 & 토크나이저 다운로드</a></li><li><a href=#peft-%eb%aa%a8%eb%8d%b8%eb%a1%9c-%eb%b3%80%ed%99%98 aria-label="PEFT 모델로 변환">PEFT 모델로 변환</a></li><li><a href=#%eb%8d%b0%ec%9d%b4%ed%84%b0%ec%85%8b-%eb%b6%88%eb%9f%ac%ec%98%a4%ea%b8%b0 aria-label="데이터셋 불러오기">데이터셋 불러오기</a></li><li><a href=#trainer-%ea%b5%ac%ed%98%84 aria-label="Trainer 구현">Trainer 구현</a></li><li><a href=#%ed%95%99%ec%8a%b5 aria-label=학습>학습</a></li></ul></li><li><a href=#%ed%95%99%ec%8a%b5-%ea%b2%b0%ea%b3%bc aria-label="학습 결과">학습 결과</a></li><li><a href=#%eb%a7%88%ec%b9%98%eb%a9%b0 aria-label=마치며>마치며</a></li></ul></div></details></div><div class=post-content><h1 id=개요>개요<a hidden class=anchor aria-hidden=true href=#개요>#</a></h1><p>글을 작성하는 현 시점 기준으로 약 2주 전 <a href=https://llama.meta.com/llama3/ target=_blank>라마3</a>가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 <a href=https://huggingface.co/beomi/Llama-3-Open-Ko-8B target=_blank>이 모델</a>을 베이스로 사용하기로 했다.</p><h1 id=학습-전-준비물>학습 전 준비물<a hidden class=anchor aria-hidden=true href=#학습-전-준비물>#</a></h1><ul><li><strong>개발 환경 (GPU 클라우드)</strong><ul><li>RTX 4090 X 1</li><li>Ubuntu 22.04</li><li>PyTorch 2.2.0</li><li>CUDA 12.1</li></ul></li><li><strong>충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러)</strong></li><li><strong>초연한 자세</strong><ul><li>어떠한 오류가 터져도 화내지 않는 강건한 정신</li></ul></li><li><strong>채팅 데이터셋</strong><ul><li><a href=https://aihub.or.kr target=_blank>AI Hub</a>에서 가져온 <strong>주제별 텍스트 일상 대화 데이터</strong></li><li>데이터 전처리</li></ul></li><li><strong>라이브러리(pypi)</strong><ul><li><a href=https://github.com/unslothai/unsloth target=_blank>unsloth</a></li><li>accelerate</li><li>transformers</li><li>trl</li><li>datasets</li><li>peft</li><li>etc.</li></ul></li></ul><h1 id=채팅-데이터셋-전처리>채팅 데이터셋 전처리<a hidden class=anchor aria-hidden=true href=#채팅-데이터셋-전처리>#</a></h1><p>JSON으로 구성된 채팅 데이터셋은 대충 전처리하면 다음과 같은 텍스트 형태가 된다.</p><pre tabindex=0><code>user: 점심 메뉴 정하신 분
bot: 누룽지 끓여 먹을까 고민
user: 나 아침 든든하게 먹으니 힘 난다
user: 아 나 점심 뭐 먹지
bot: 점심은 사모님이 주심 후후
user: 나 점심 내장국밥 픽
user: **가 오늘 만들어야 함?
bot: 누룽지는 간식으로 먹어야징
user: 돼지국밥으로 바꿀까
user: 누룽지 맛있겠어... 나두
bot: 여기 먹을 거 개많음 ㄷㄷ
user: 아니면 두루치기 먹을가
user: 국밥은 언제나 맛있음
bot: 떡볶이 이런 거 먹을 수 있음 키키
user: 먹는 게 제이 좋아!
user: 국밥 1그릇 배달 안 해주겠지 ㅠ
bot: 수제비랑 밀키트 다 나왕
user: 떡볶이 파는 데가 없음 여기
&lt;|endoftext|&gt;
</code></pre><p>여기서 문제가 하나 발생한다. 우리는 로봇과 1대1로 대화하기를 원하는데, 이 데이터셋은 화자가 최대 3명이기 때문에 1번 화자는 <code>user</code>, 2번 화자는 <code>bot</code>, 3번 화자는 다시 <code>user</code>로 치환해버려서 user는 이중인격자가 되는 것을 확인할 수 있다. 이 부분은 학습이 완료된 후 알아채서 다음에는 <code>user1, user2, user3, ...</code>이런 식으로 시도해보려고 한다.<br>또한 이름도 개인정보 보호 차원에서 별 모양 기호(asterisk, *)로 뜨는데, 적당하게 &lt;name>으로 바꿨으면 더 좋았을 것 같다.<br>마지막으로 오타가 상당히 많은데, 오타 좀 내면 더욱 사람과 비슷해지지 않을까 하는 생각이 들어 오타 교정은 수행하지 않았다. 사실 몇만 줄의 텍스트 파일을 맞춤법 검사기를 돌려 일일이 수정하는 것도 미친 짓이라고 생각했다.</p><h1 id=학습-코드>학습 코드<a hidden class=anchor aria-hidden=true href=#학습-코드>#</a></h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span>print(torch<span style=color:#f92672>.</span>__version__)
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tqdm.auto <span style=color:#f92672>import</span> tqdm
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> unsloth <span style=color:#f92672>import</span> FastLanguageModel
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> trl <span style=color:#f92672>import</span> SFTTrainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TrainingArguments
</span></span></code></pre></div><p>먼저 필요한 모듈을 import한다. 여기서 unsloth 모듈은 뭐하는 놈일까?</p><h2 id=unsloth는-무엇인가>Unsloth는 무엇인가<a hidden class=anchor aria-hidden=true href=#unsloth는-무엇인가>#</a></h2><p>unsloth는 여러 언어모델들을 파인튜닝 시 2배~5배 빠르게, 메모리 사용량을 80%까지 줄일 수 있다고 주장하는 라이브러리다. 실제로 vram 사용량을 꽤 줄였던 것으로 기억한다.</p><table><thead><tr><th>1 A100 40GB</th><th>🤗Hugging Face</th><th>Flash Attention</th><th>🦥Unsloth Open Source</th><th>🦥<a href=https://unsloth.ai/pricing target=_blank>Unsloth Pro</a></th></tr></thead><tbody><tr><td>Alpaca</td><td>1x</td><td>1.04x</td><td>1.98x</td><td><strong>15.64x</strong></td></tr><tr><td>LAION Chip2</td><td>1x</td><td>0.92x</td><td>1.61x</td><td><strong>20.73x</strong></td></tr><tr><td>OASST</td><td>1x</td><td>1.19x</td><td>2.17x</td><td><strong>14.83x</strong></td></tr><tr><td>Slim Orca</td><td>1x</td><td>1.18x</td><td>2.22x</td><td><strong>14.82x</strong></td></tr></tbody></table><p>위 표는 unsloth의 깃허브 readme에서 가져온 벤치마크 결과이다. 최대 2배 빠르다는 건 이해할 수 있지만, 아무리 봐도 14~15배 빨라진다는 unsloth pro는 그짓말 같긴 하다.</p><p>아무튼 unsloth는 파이토치 버전, 쿠다 버전, GPU Compute Capability에 따라 설치 명령어가 약간씩 다르니 꼭 <a href=https://github.com/unslothai/unsloth target=_blank>README</a>를 참고하여 설치하자.</p><h2 id=모델--토크나이저-다운로드>모델 & 토크나이저 다운로드<a hidden class=anchor aria-hidden=true href=#모델--토크나이저-다운로드>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model, tokenizer <span style=color:#f92672>=</span> FastLanguageModel<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#39;beomi/Llama-3-Open-Ko-8B&#39;</span>, max_seq_length<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>, dtype<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, load_in_4bit<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p><code>unsloth</code>에서 제공하는 <code>FastLanguageModel</code>을 사용하여 <code>transformers</code> 라이브러리와 비슷한 방식으로 모델을 다운로드 받을 수 있다.<br>그리고 <code>load_in_4bit</code> 인자를 사용하여 모델을 4비트 양자화한 상태로 불러온다. 양자화를 하지 않으면 나중에 학습할 때 VRAM 점유율이 4090의 24GB를 초과하여 OOM(CUDA Out Of Memory) 오류가 터진다. 다만 양자화로 인해 모델의 성능 저하도 있는 것 같다. <code>max_seq_length</code>는 입력 토큰의 최대 길이이다. 1024 토큰이 좀 작을 수도 있지만, OOM이 무서워서 일단 1024로 했다. 나중에 RoPE Scaling이라는 기술을 활용하면 모델의 context size를 파인튜닝 없이 늘릴 수 있다고 한다.</p><h2 id=peft-모델로-변환>PEFT 모델로 변환<a hidden class=anchor aria-hidden=true href=#peft-모델로-변환>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model <span style=color:#f92672>=</span> FastLanguageModel<span style=color:#f92672>.</span>get_peft_model(
</span></span><span style=display:flex><span>    model, 
</span></span><span style=display:flex><span>    r<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>    target_modules <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#34;q_proj&#34;</span>, <span style=color:#e6db74>&#34;k_proj&#34;</span>, <span style=color:#e6db74>&#34;v_proj&#34;</span>, <span style=color:#e6db74>&#34;o_proj&#34;</span>,
</span></span><span style=display:flex><span>                  <span style=color:#e6db74>&#34;gate_proj&#34;</span>, <span style=color:#e6db74>&#34;up_proj&#34;</span>, <span style=color:#e6db74>&#34;down_proj&#34;</span>,],
</span></span><span style=display:flex><span>    lora_alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>,
</span></span><span style=display:flex><span>    lora_dropout<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>    bias<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;none&#34;</span>,
</span></span><span style=display:flex><span>    use_gradient_checkpointing<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;unsloth&#34;</span>,
</span></span><span style=display:flex><span>    max_seq_length<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>,
</span></span><span style=display:flex><span>    use_rslora<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>    loftq_config<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>PEFT(Parameter Efficient Fine-Tuning)는 Huggingface 커뮤니티에서 개발 중인 라이브러리다. PEFT는 모델의 전체 파라미터를 학습하는 대신, LoRA와 같은 기술을 사용하여 필요한 작은 수의 파라미터만 학습해서 컴퓨팅 비용을 줄일 수 있는 모듈이다. 기존 학습 코드를 조금만 수정하면 효율적으로 학습이 가능하다는 점이 장점이다.<br>Unsloth는 PEFT 모듈 + $\alpha$를 활용해서 속도와 메모리 사용량을 줄이는 방식이기 때문에 두 라이브러리는 다르다.</p><p>Unsloth에서 제공하는 <code>FastLanguageModel</code>에서 모델을 PEFT 모델로 변환할 수 있다. 내부적으로 LoRA를 사용하는 듯하다. LoRA 기술에 대해서는 기본적인 동작 원리만 이해하고 자세히는 알지 못해서 나중에 따로 공부해보고 싶다.</p><h2 id=데이터셋-불러오기>데이터셋 불러오기<a hidden class=anchor aria-hidden=true href=#데이터셋-불러오기>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dataset <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#39;json&#39;</span>, data_files<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;chat.json&#39;</span>, split<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;train&#39;</span>)
</span></span></code></pre></div><p>huggingface <code>datasets</code> 모듈으로 아까 전처리한 텍스트 파일을 대화 주제 단위로 자르고, 다음과 같은 JSON 형식으로 변환해서 <code>load_dataset</code> 함수로 불러왔다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>[
</span></span><span style=display:flex><span>	{
</span></span><span style=display:flex><span>		<span style=color:#f92672>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;user: 점심 메뉴 정하신 분 bot: 누룽지 끓여 먹을까 고민...[이하생략]&gt;&#34;</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><h2 id=trainer-구현>Trainer 구현<a hidden class=anchor aria-hidden=true href=#trainer-구현>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>trainer <span style=color:#f92672>=</span> SFTTrainer(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>    train_dataset<span style=color:#f92672>=</span>dataset,
</span></span><span style=display:flex><span>    dataset_text_field<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;text&#39;</span>,
</span></span><span style=display:flex><span>    max_seq_length<span style=color:#f92672>=</span><span style=color:#ae81ff>1024</span>,
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>=</span>tokenizer,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>=</span>TrainingArguments(
</span></span><span style=display:flex><span>        per_device_train_batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>        gradient_accumulation_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>        warmup_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>,
</span></span><span style=display:flex><span>        bf16<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        logging_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>,
</span></span><span style=display:flex><span>        output_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;results&#39;</span>,
</span></span><span style=display:flex><span>        optim<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;adamw_8bit&#39;</span>,
</span></span><span style=display:flex><span>    ),
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>trl에서 제공하는 SFTTrainer(Supervised Fine-Tuning Trainer)를 사용하면 쉽게 파인튜닝할 수 있다. 여기서 중요한 코드가 <code>TrainingArguments</code> 부분이라고 생각하는데, 학습에 필요한 learning rate 등을 TrainingArguments에 인자로 넘기는 부분이기 때문이다. <code>per_device_train_batch_size</code> 대신 <code>auto_find_batch_size=True</code>를 쓰면 자동으로 최적의 batch size를 찾아준다. 사실 <code>TrainingArguments</code>는 unsloth의 readme를 귀찮아서 대충 그냥 갖다 쓴거라 뭔지는 나도 잘 모른다.</p><h2 id=학습>학습<a hidden class=anchor aria-hidden=true href=#학습>#</a></h2><p>이제 학습은 굉장히 간단해진다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>trainer<span style=color:#f92672>.</span>train()
</span></span></code></pre></div><p>끝.</p><p>Unsloth가 편리한게 다음과 같이 코드를 입력하면 자동으로 llama.cpp에서 돌아가는 GGUF 파일 형식으로 변환하고 양자화까지 해준다는 점이다.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#f92672>.</span>save_pretrained_gguf(<span style=color:#e6db74>&#34;dir&#34;</span>, tokenizer, quantization_method <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;q4_k_m&#34;</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>save_pretrained_gguf(<span style=color:#e6db74>&#34;dir&#34;</span>, tokenizer, quantization_method <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;q8_0&#34;</span>)
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>save_pretrained_gguf(<span style=color:#e6db74>&#34;dir&#34;</span>, tokenizer, quantization_method <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;f16&#34;</span>)
</span></span></code></pre></div><h1 id=학습-결과>학습 결과<a hidden class=anchor aria-hidden=true href=#학습-결과>#</a></h1><p>이제 챗봇과 대화를 해보자.</p><blockquote><p><strong>대화 기록</strong></p><pre tabindex=0><code class=language-raw data-lang=raw>user: 안녕
bot: 안녕
user: 오늘 날씨 좋다 ㅎㅎ
bot: 오늘 날씨가 너무 좋아 키키
user: 밖에 나가자
bot: 그러게 완전 가을이야~
user: 여름인데?
bot: 여름이 아니지 않나?
user: 여름맞는데?
bot: 아직 여름인 거 같은데?
user: 지금 내 말이 틀렸다고 주장하는 거니?
bot: 그렇다고 할 수는 없잖아~ 나도 좋아 키키
</code></pre></blockquote><p>내가 파인튜닝을 너무 대충 해서 그런지는 모르겠는데, 대화를 하면 할 수록 배가 산으로 가는 현상을 볼 수 있다.<br>아니면 약 4MB 용량의 적은 데이터셋으로 학습을 진행해서 성능이 구린 이유도 있는 것 같다.</p><h1 id=마치며>마치며<a hidden class=anchor aria-hidden=true href=#마치며>#</a></h1><p>대략 2년 전부터 진짜 사람같은 챗봇을 만들고 싶어서 여러가지 시도를 해보고 있다. ChatGPT 등 성능이 좋은 모델의 api를 사용하면 되지 않느냐고 생각할 수 있지만, API를 사용하는 방식은 내 성미에 맞지 않기도 하고 gpt 특유의 어색한 말투가 너무 부자연스러워서 대화에 몰입할 수 없었다.<br>재미있는 건 1년 전에 EleutherAI의 <code>polyglot-ko</code>라는 pretrained 모델을 사용해서 파인튜닝한 결과물이 지금보다 좋았다고 느꼈다는 사실이다. 아무래도 polyglot-ko는 완전히 처음부터 한국어 데이터로 pretraining한 반면, 라마와 같은 모델은 영어의 비중이 높고, 한국어가 거의 없는 데이터셋으로 학습 후 누군가 한국어로 다시 파인튜닝했기 때문에 한국어 능력이 부족하지 않았나 추측해본다.<br>라마와 같은 pretrained 모델 하나를 만드려면 엄청난 비용과 시간이 든다. 그럼에도 불구하고 2022년에 공개된 polyglot-ko 이후 처음부터 한국어로 학습한 모델이 전무하다고 할 수 있을 정도로 현재 한국어 LLM 생태계는 기존 영어 모델을 파인튜닝하고, 다시 파인튜닝하는 부분에 초점이 맞춰져있는 것 같다. 그래서 나는 네이버 등의 회사가 비교적 작은 모델인 한국어 sLLM을 공개했으면 좋겠다는 바람이 있다.</p><blockquote><p>개인적인 의견이므로 부정확할 수 있으니 참고하지 마시오.</p></blockquote></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.hee.blue/tags/llama/>LLaMA</a></li><li><a href=https://blog.hee.blue/tags/llm/>LLM</a></li><li><a href=https://blog.hee.blue/tags/ai/>AI</a></li><li><a href=https://blog.hee.blue/tags/fine-tuning/>Fine-Tuning</a></li><li><a href=https://blog.hee.blue/tags/unsloth/>Unsloth</a></li><li><a href=https://blog.hee.blue/tags/conversational/>Conversational</a></li></ul></footer></article></main><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>