<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>LLM on J.log</title><link>https://blog.hee.blue/tags/llm/</link><description>Recent content in LLM on J.log</description><generator>Hugo -- 0.125.3</generator><language>ko-kr</language><lastBuildDate>Fri, 10 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.hee.blue/tags/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>Efficient LLaMA3 Fine-Tuning with Unsloth</title><link>https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/</link><pubDate>Fri, 10 May 2024 00:00:00 +0000</pubDate><guid>https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/</guid><description>개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.
학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.1 충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러) 초연한 자세 어떠한 오류가 터져도 화내지 않는 강건한 정신 채팅 데이터셋 AI Hub에서 가져온 주제별 텍스트 일상 대화 데이터 데이터 전처리 라이브러리(pypi) unsloth accelerate transformers trl datasets peft etc.</description></item></channel></rss>