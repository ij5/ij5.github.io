<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>AI on J.log</title><link>https://blog.hee.blue/tags/ai/</link><description>Recent content in AI on J.log</description><generator>Hugo -- 0.125.3</generator><language>ko-kr</language><lastBuildDate>Mon, 13 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.hee.blue/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Rust로 감성분석 AI 구현하기 (LSTM, GRU)</title><link>https://blog.hee.blue/posts/ai/sentiment-ai-implementation-with-rust/</link><pubDate>Mon, 13 May 2024 00:00:00 +0000</pubDate><guid>https://blog.hee.blue/posts/ai/sentiment-ai-implementation-with-rust/</guid><description>개요 파이썬은 내가 제일 많이 사용하는 언어 중 하나이다. 또한 파이썬은 PyTorch, Tensorflow, Keras 등 굉장히 편리하고 사용하기 쉬운 라이브러리가 많다. 하지만 대부분의 파이썬 머신러닝 라이브러리의 중심 부분은 파이썬으로 구현되지 않고 C, C++ 등의 저수준 언어로 구현되어있다. 파이썬은 인공신경망 등의 알고리즘을 실행시키기에는 너무나도 느리기 때문이다.
PyTorch는 인공지능 개발에 필수적인 라이브러리라고 할 수 있다. 파이토치에서 제공하는 수백, 혹은 수천 개의 머신러닝을 위한 API는 코드 몇 줄로 모델을 만들거나, 학습시키고 추론하기 위한 다양한 함수들을 제공한다.</description></item><item><title>Efficient LLaMA3 Fine-Tuning with Unsloth</title><link>https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/</link><pubDate>Fri, 10 May 2024 00:00:00 +0000</pubDate><guid>https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/</guid><description>개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.
학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.1 충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러) 초연한 자세 어떠한 오류가 터져도 화내지 않는 강건한 정신 채팅 데이터셋 AI Hub에서 가져온 주제별 텍스트 일상 대화 데이터 데이터 전처리 라이브러리(pypi) unsloth accelerate transformers trl datasets peft etc.</description></item></channel></rss>