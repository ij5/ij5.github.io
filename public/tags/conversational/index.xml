<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Conversational on J.log</title><link>https://blog.hee.blue/tags/conversational/</link><description>Recent content in Conversational on J.log</description><generator>Hugo -- 0.125.3</generator><language>ko-kr</language><lastBuildDate>Fri, 10 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.hee.blue/tags/conversational/index.xml" rel="self" type="application/rss+xml"/><item><title>Efficient LLaMA3 Fine-Tuning with Unsloth</title><link>https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/</link><pubDate>Fri, 10 May 2024 00:00:00 +0000</pubDate><guid>https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/</guid><description>개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 나는 새로운 pretrained LLM이 공개되었을 때 채팅 데이터를 학습시키지 않으면 입 안에 가시가 돋아 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 라마3가 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.
학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.</description></item></channel></rss>