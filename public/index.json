[{"content":"개요 요즘 ASR(Automatic Speech Recognition)에 관심이 생겼다. 몇 년 전까지만 해도 오픈소스 한국어 Speech To Text 모델 개발이 활발하게 이루어졌지만, 현재는 대부분의 저장소가 아카이빙되거나, 개발이 중단되었다. 남아있는 건 OpenAI의 Whisper 정도인데 Whisper는 CPU 추론이 너무너무너무너무너무너무 느려서 내가 원하는 조건에 부합하지 않았다.\n","permalink":"https://blog.hee.blue/posts/ai/asr-wav2vec2/","summary":"개요 요즘 ASR(Automatic Speech Recognition)에 관심이 생겼다. 몇 년 전까지만 해도 오픈소스 한국어 Speech To Text 모델 개발이 활발하게 이루어졌지만, 현재는 대부분의 저장소가 아카이빙되거나, 개발이 중단되었다. 남아있는 건 OpenAI의 Whisper 정도인데 Whisper는 CPU 추론이 너무너무너무너무너무너무 느려서 내가 원하는 조건에 부합하지 않았다.","title":"ASR이란 무엇인가 - Wav2Vec 2.0"},{"content":"지능형 기차 (2455) 문제 최근에 개발된 지능형 기차가 1번역(출발역)부터 4번역(종착역)까지 4개의 정차역이 있는 노선에서 운행되고 있다. 이 기차에는 타거나 내리는 사람 수를 자동으로 인식할 수 있는 장치가 있다. 이 장치를 이용하여 출발역에서 종착역까지 가는 도중 기차 안에 사람이 가장 많을 때의 사람 수를 계산하려고 한다. 단, 이 기차를 이용하는 사람들은 질서 의식이 투철하여, 역에서 기차에 탈 때, 내릴 사람이 모두 내린 후에 기차에 탄다고 가정한다.\n내린 사람 수 탄 사람 수 1번역(출발역) 0 32 2번역 3 13 3번역 28 25 4번역(종착역) 39 0 예를 들어, 위와 같은 경우를 살펴보자. 이 경우, 기차 안에 사람이 가장 많은 때는 2번역에서 3명의 사람이 기차에서 내리고, 13명의 사람이 기차에 탔을 때로, 총 42명의 사람이 기차 안에 있다.\n이 기차는 다음 조건을 만족하면서 운행된다고 가정한다.\n기차는 역 번호 순서대로 운행한다. 출발역에서 내린 사람 수와 종착역에서 탄 사람 수는 0이다. 각 역에서 현재 기차에 있는 사람보다 더 많은 사람이 내리는 경우는 없다. 기차의 정원은 최대 10,000명이고, 정원을 초과하여 타는 경우는 없다. 4개의 역에 대해 기차에서 내린 사람 수와 탄 사람 수가 주어졌을 때, 기차에 사람이 가장 많을 때의 사람 수를 계산하는 프로그램을 작성하시오.\n입력 각 역에서 내린 사람 수와 탄 사람 수가 빈칸을 사이에 두고 첫째 줄부터 넷째 줄까지 역 순서대로 한 줄에 하나씩 주어진다.\n출력 첫째 줄에 최대 사람 수를 출력한다.\n예제 입력 0 32 3 13 28 25 39 0 예제 출력 42 풀이 passengers = 0 # 현재 기차에 탄 승객 peak = [] # 각 정차 역 마다 내리고 탑승한 이후의 값 저장 for _ in range(4): leave, enter = map(int, input().split()) passengers -= leave passengers += enter peak.append(passengers) # 리스트에 현재 기차 내 승객을 추가 print(max(peak)) # 열차에 사람이 가장 많았을 때의 승객 수를 출력 소수 찾기 (1978) 문제 주어진 수 N개 중에서 소수가 몇 개인지 찾아서 출력하는 프로그램을 작성하시오.\n입력 첫 줄에 수의 개수 N이 주어진다. N은 100이하이다. 다음으로 N개의 수가 주어지는데 수는 1,000 이하의 자연수이다.\n출력 주어진 수들 중 소수의 개수를 출력한다.\n예제 입력 4 1 3 5 7 예제 출력 3 풀이 import math # 내장 라이브러리 포함 n = int(input()) # 변수 n 입력 x = list(map(int, input().split())) # 입력받은 n개의 수를 리스트에 입력 count = 0 # 주어진 수들 중 소수의 개수 for i in x: if i \u0026lt;= 1: # 1은 소수가 아니므로 i가 1일 때 continue continue prime = True # i를 소수로 가정 # 정수 i의 제곱근 이하의 값까지만 검사하면 나머지는 검사할 필요가 없다. # 예를 들어 i가 24라면 i의 제곱근은 약 4.898이다. # 이때, i의 제곱근과 곱해서 i가 되는 약수들을 나열해보면 다음과 같다. # 1 * 24 # 2 * 12 # 3 * 8 # 4 * 6 # 4.898 * 4.898 # 6 * 4 # 8 * 3 # 12 * 2 # 24 * 1 # 여기서 i의 제곱근인 약 4.898을 기준으로 약수들의 곱이 대칭이 되는 것을 확인할 수 있다. # 이러한 원리를 이용하면 i까지 반복할 필요 없이 좀 더 빠르게 소수를 구하는 것이 가능하다. for j in range(2, int(math.sqrt(i)) + 1): # 2 ~ i의 제곱근까지 반복하기 if i % j == 0: # 만약 i가 j로 나누어 떨어질 때 i가 소수라는 가정은 거짓이 됨 prime = False break # 소수가 아님을 판별했으므로 반복문 종료 if prime: count += 1 # 만약 i가 소수면 1을 더한다. print(count) # 최종적으로 소수의 개수 출력 ","permalink":"https://blog.hee.blue/posts/algorithm/python-solve-100/","summary":"지능형 기차 (2455) 문제 최근에 개발된 지능형 기차가 1번역(출발역)부터 4번역(종착역)까지 4개의 정차역이 있는 노선에서 운행되고 있다. 이 기차에는 타거나 내리는 사람 수를 자동으로 인식할 수 있는 장치가 있다. 이 장치를 이용하여 출발역에서 종착역까지 가는 도중 기차 안에 사람이 가장 많을 때의 사람 수를 계산하려고 한다. 단, 이 기차를 이용하는 사람들은 질서 의식이 투철하여, 역에서 기차에 탈 때, 내릴 사람이 모두 내린 후에 기차에 탄다고 가정한다.\n내린 사람 수 탄 사람 수 1번역(출발역) 0 32 2번역 3 13 3번역 28 25 4번역(종착역) 39 0 예를 들어, 위와 같은 경우를 살펴보자.","title":"파이썬 100제 풀기"},{"content":"개요 파이썬은 내가 제일 많이 사용하는 언어 중 하나이다. 또한 파이썬은 PyTorch, Tensorflow, Keras 등 굉장히 편리하고 사용하기 쉬운 라이브러리가 많다. 하지만 대부분의 파이썬 머신러닝 라이브러리의 중심 부분은 파이썬으로 구현되지 않고 C, C++ 등의 저수준 언어로 구현되어있다. 파이썬은 인공신경망 등의 알고리즘을 실행시키기에는 너무나도 느리기 때문이다.\nPyTorch는 인공지능 개발에 필수적인 라이브러리라고 할 수 있다. 파이토치에서 제공하는 수백, 혹은 수천 개의 머신러닝을 위한 API는 코드 몇 줄로 모델을 만들거나, 학습시키고 추론하기 위한 다양한 함수들을 제공한다. 실제로 구글 트렌드의 관심도 변화를 보면 2022년 6월을 기준으로 파이토치가 텐서플로우를 앞서는 모습을 보인다.\n그런데, 파이토치나 텐서플로우는 지원하는 기능들이 워낙 많다보니 cpu 환경에서 파이토치를 설치하려면 최소 600MB의 데이터를 다운로드 해야 한다. 그래서 나는 모델 학습은 고사양 컴퓨터에서, 모델 추론은 보다 저사양의 컴퓨터에서 하는 방식이 좀 더 효율적이라고 생각했다. 일반적으로 딥러닝 모델은 추론보다 학습에 더 많은 컴퓨팅 리소스를 사용하기 때문이다.\nHuggingFace는 Tokenizers 라이브러리를 러스트로 구현하였고, 작년부터 Candle 프로젝트를 개발하고 있는데, 이 Candle은 러스트로 만든 머신러닝 프레임워크이다. 그리고 함수 이름과 인덱싱 등을 파이토치와 비슷하게 만들어놔서, 기존 파이토치로 구현한 모델을 러스트로 다시 구현하기에 어려움이 없도록 설계했다는 점이 가장 큰 장점인 것 같다. 파이토치와 비교했을 때 매우 가볍고, CPU 환경에서 작은 사이즈의 단일 실행 파일로 컴파일되어 상당히 마음에 들었다. 그래서 Candle을 사용하여 네이버 영화 리뷰(NSMC) 분류 모델을 만들어 보려고 한다.\n학습 환경 Ubuntu 22.04 Python 3.10.14 PyTorch 2.2.0+cu121 Pandas Tokenizers Numpy tqdm safetensors 추론 환경 Windows 11 Rust 1.78.0 candle 0.5.1 tokenizers 0.19.1 anyhow 1.0.83 데이터셋 준비 네이버 영화 리뷰 데이터셋(NSMC, Naver Sentiment Movie Corpus)은 여기에서 다운로드 받을 수 있다. 필요한 파일은 ratings_train.txt이다.\n모델 구현 및 학습 Python 토크나이저 학습 제일 먼저 토크나이저를 학습시켜야 한다.\nfrom tokenizers import ByteLevelBPETokenizer 나는 한글 데이터를 학습시킬 것이므로 BPE 토크나이저가 아무래도 더 적합하다고 생각했다.\ntokenizer = ByteLevelBPETokenizer() tokenizer.train([\u0026#39;ratings_train.txt\u0026#39;], vocab_size=10000) 토크나이저의 vocab 크기를 1만개로로 정의하고 \u0026ldquo;데이터셋 준비\u0026rdquo; 에서 다운받은 ratings_train.txt 파일을 인자로 넘기면 간단하게 학습이 가능하다. 학습하는 데 대략 30초 정도 소요된다.\ntokenizer.encode(\u0026#34;재미있는 영화군요\u0026#34;).ids 잘 학습이 됐는지 테스트하기 위해 아무 문장이나 인코딩 해본다.\n출력은 다음과 같다.\n[7457, 20609] tokenizer.save(\u0026#34;tokenizer.json\u0026#34;) 학습이 완료된 토크나이저를 JSON 형식으로 저장한다.\n모델링 import torch import torch.nn as nn 다음으로 모델링에 필요한 라이브러리를 불러온다. 나는 LSTM, GRU와 같은 인공신경망 알고리즘이 필요하기 때문에 torch.nn 패키지를 nn만 쳐도 쉽게 사용할 수 있도록 하였다.\nhidden_dim = 128 output_dim = 128 num_labels = 2 class TextClassifier(nn.Module): def __init__(self): super(TextClassifier, self).__init__() self.embedding = nn.Embedding(10000, hidden_dim) self.gru = nn.GRU(hidden_dim, output_dim, batch_first=True) self.ln1 = nn.Linear(outptu_dim, num_labels) def forward(self, x): embedded = self.embedding(x) gru_out, hidden = self.gru(embedded) last_hidden = hidden.squeeze(0) logits = self.ln1(last_hidden) return logits 모델 구현 부분은 간단하다. TextClassifier 클래스가 nn.Module 클래스를 상속하도록 하고, 레이어들을 정의하면 된다. 제일 먼저 embedding 레이어는 아까 학습한 토크나이저에 1만개의 vocab이 존재하므로 입력 부분은 토크나이저와 같이 10000을 인자로 넘겼다. 임베딩을 거쳐서 GRU(혹은 LSTM) 레이어에 값이 전달되고, 최종적으로 1과 0(긍정 혹은 부정)을 출력해야 하기 때문에 Linear 레이어의 출력을 2로 설정했다.\nGRU의 batch_first 부분은 (seq, batch, feature)로 된 데이터를 (batch, seq, feature)처럼 배치 사이즈가 맨 앞으로 오게 하라는 뜻이다.\n그리고 forward 함수에는 __init__에서 정의했던 레이어들에 데이터를 전달하는 코드를 작성한다.\n모델 학습 import torch import pandas as pd import torch.nn as nn from tokenizers import Tokenizer from tqdm.auto import tqdm import numpy as np from torch.utils.data import TensorDataset, DataLoader from safetensors.torch import save_file 필요한 라이브러리들을 불러온다.\ndf = pd.read_csv(\u0026#39;ratings_train.txt\u0026#39;, sep=\u0026#39;\\t\u0026#39;) 이전에 다운받은 텍스트 파일은 사실 raw 형식이 아니라 tsv 형식이었다.\ndf.head()를 실행하면 다음과 같이 영화 리뷰 앞부분을 미리 볼 수 있다.\nid document label 9976970 아 더빙.. 진짜 짜증나네요 목소리 0 3819312 흠\u0026hellip;포스터보고 초딩영화줄\u0026hellip;.오버연기조차 가볍지 않구나 1 10265843 너무재밓었다그래서보는것을추천한다 0 9045019 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정 0 6483659 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서\u0026hellip; 1 tokenizer = Tokenizer.from_file(\u0026#39;tokenizer.json\u0026#39;) 아까 저장된 tokenizer.json 파일을 불러온다.\ndataset = [tokenizer.encode(x).ids for x, y in zip(list(df[\u0026#39;document\u0026#39;]), list(df[\u0026#39;label\u0026#39;])) if str(x) != \u0026#39;nan\u0026#39; and str(y) != \u0026#39;nan\u0026#39;] labels = [y for x, y in zip(list(data[\u0026#39;document\u0026#39;]), list(data[\u0026#39;label\u0026#39;])) if str(x) != \u0026#39;nan\u0026#39; and str(y) != \u0026#39;nan\u0026#39;] 귀찮아서 한줄로 짰더니 이 꼬라지가 나버렸다. 대충 데이터셋의 document 부분이랑 label 부분을 가져오면서 둘 다 값이 존재하면 리스트에 추가하는 코드라고 이해하면 된다.\ndataset[0]을 입력하면 [333, 2547, 262, 617, 4518, 480, 4492]와 같이 정수로 인코딩된 문장이 나타나고, labels[0]을 입력하면 1 또는 0이 출력된다.\nmax([len(x) for x in dataset]) 모든 문장의 최대 길이를 출력해보면 117이 나온다. 즉 정수로 인코딩 된 데이터셋은 길이가 117을 초과하지 않는다는 뜻이다.\ndef pad_sequences(sentences, max_len): features = np.zeros((len(sentences), max_len), dtype=int) for i, sentence in enumerate(sentences): if len(sentence) != 0: features[i, :len(sentence)] = np.array(sentence)[:max_len] return features padded = pad_sequences(dataset, 128) 넉넉하게 128로 패딩하였다. 패딩 후 train_dataset의 맨 첫 번째 항목은 아래와 비슷하게 나올 것이다.\narray([ 333, 2547, 262, 617, 4518, 480, 4492, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) train_dataset = TensorDataset(torch.tensor(padded), torch.tensor(labels)) train_loader = DataLoader(train_dataset, batch_size=32) TensorDataset을 사용해 정수 리스트로 된 데이터셋을 파이토치에서 사용하는 tensor 형식으로 변환한다.\n그 다음 병렬 학습을 위해 DataLoader에 배치 사이즈를 인자로 넣어 몇 개의 데이터를 병렬로 처리할 건지 설정한다.\nmodel = TextClassifier() criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) num_epochs = 6 model.to(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) \u0026ldquo;모델링\u0026quot;에서 구현한 TextClassifier 모델을 생성하고, loss 함수와 optimizer, 반복 학습 횟수(num_epochs)를 정한다. 만약 cuda 지원 그래픽카드가 있다면, model.to를 사용하여 GPU에 모델을 올릴 수 있다.\nfor epoch in range(num_epochs): train_loss = 0 model.train() for i, (x, y) in tqdm(enumerate(train_loader)): x, y = x.to(\u0026#39;cuda\u0026#39;), y.to(\u0026#39;cuda\u0026#39;) logits = model(x) loss = criterion(logits, y) optimizer.zero_grad() loss.backward() optimizer.step() train_loss += loss.item() if i % 50 == 0 and i != 0: tqdm.write(f\u0026#34;Loss: {train_loss / i}\\r\u0026#34;, end=\u0026#39;\u0026#39;) print(f\u0026#34;Epoch Loss: {train_loss / len(train_loader)}\u0026#34;) 드디어 학습을 진행한다. 50스텝, 1에폭마다 loss를 출력하게 구현하였다.\n그러면 다음과 같이 결과가 출력되는데, 마음이 편안해진다.\nEpoch Loss: 0.44622183628793516 Epoch Loss: 0.3024691305053341 Epoch Loss: 0.23970229042306188 Epoch Loss: 0.18055053850287833 Epoch Loss: 0.13453298333506641 Epoch Loss: 0.10856294956053496 마지막으로 학습한 모델을 safetensors 형식으로 내보내면 끝이다.\nsave_file(model.state_dict(), \u0026#34;nsmc.safetensors\u0026#34;) Rust 이미 학습을 진행했고, 모델 파일까지 얻었으므로 Rust 코드에서는 모델만 구현하면 된다.\ncargo new classification과 같이 프로젝트를 새로 만들고, Cargo.toml 파일의 [dependencies] 항목에 다음과 같이 라이브러리를 추가한다.\n[dependencies] anyhow = \u0026#34;1.0.83\u0026#34; candle-core = { git = \u0026#34;https://github.com/huggingface/candle.git\u0026#34;, version = \u0026#34;0.5.1\u0026#34; } candle-nn = { git = \u0026#34;https://github.com/huggingface/candle.git\u0026#34;, version = \u0026#34;0.5.1\u0026#34; } tokenizers = \u0026#34;0.19.1\u0026#34; 버전은 달라질 수 있으므로 공식 문서를 참조하여 최신 버전 사용을 권장한다.\nCargo.toml에 다음 코드를 추가한다.\n[features] cuda = [\u0026#34;candle-core/cuda\u0026#34;, \u0026#34;candle-nn/cuda\u0026#34;] 만약 시스템이 CUDA를 지원한다면, feature 플래그를 통해 candle의 cuda를 활성화하는 부분이다.\n프로젝트 내의 main.rs 파일의 내용을 전부 지우고 처음부터 시작하는 것이 편하다.\nuse std::io::Write; use anyhow::Error as E; use anyhow::Result; use candle_core::{DType, Device, Tensor}; use candle_nn as nn; use nn::{Module, VarBuilder, RNN}; use tokenizers::Tokenizer; 필요한 crate를 추가한다.\n#[derive(Clone)] struct TextCLassifier { embedding: nn::Embedding, gru: nn::GRU, ln1: nn::Linear, device: Device, } 파이썬과 같이 TextClassifier 모델 구조를 정의한다.\nimpl TextCLassifier { pub fn new(vs: VarBuilder) -\u0026gt; Result\u0026lt;Self\u0026gt; { let embedding = nn::embedding(10000, 128, vs.pp(\u0026#34;embedding\u0026#34;))?; let gru = nn::gru(128, 128, Default::default(), vs.pp(\u0026#34;gru\u0026#34;))?; let ln1 = nn::linear(128, 2, vs.pp(\u0026#34;ln1\u0026#34;))?; let device = Device::cuda_if_available(0)?; return Ok(Self { embedding, ln1, gru, device, }); } pub fn forward(self, xs: \u0026amp;Tensor) -\u0026gt; Result\u0026lt;Tensor\u0026gt; { let xs = self.embedding.forward(xs)?; let mut gru_states = vec![self.gru.zero_state(1)?]; for x in \u0026amp;xs.squeeze(0)?.to_vec2::\u0026lt;f32\u0026gt;()? { let state = self.gru.step( \u0026amp;Tensor::from_vec(x.clone(), (1, x.len()), \u0026amp;self.device)?, \u0026amp;gru_states.last().unwrap(), )?; gru_states.push(state); } let xs = gru_states.last().unwrap().h(); let xs = self.ln1.forward(\u0026amp;xs)?; Ok(xs) } } 파이썬의 __init__과 같이 new 함수를 정의하고, 파이썬에서 구현한 모델에 맞춰 레이어 등을 똑같이 구현해야 한다.\nforward 함수의 내용은 비슷하다.\n추가로 터미널 내에서 입력 \u0026amp; 추론하려면 다음 코드를 impl TextClassifier 블록 안에 넣으면 된다.\npub fn interaction(\u0026amp;mut self, tokenizer: Tokenizer, device: \u0026amp;Device) -\u0026gt; Result\u0026lt;()\u0026gt; { loop { let mut line = String::new(); print!(\u0026#34;영화 리뷰를 입력하세요: \u0026#34;); std::io::stdout().flush()?; std::io::stdin().read_line(\u0026amp;mut line)?; if line == \u0026#34;q\u0026#34; || line == \u0026#34;exit\u0026#34; || line == \u0026#34;quit\u0026#34; { break; } let encoded = tokenizer.encode(line, false).map_err(E::msg)?; let data = Tensor::new(vec![encoded.get_ids()], device)?; let result = self.clone().forward(\u0026amp;data)?; let result = result.argmax(1)?.to_vec1::\u0026lt;u32\u0026gt;()?; if result[0] == 0 { println!(\u0026#34;부정적인 리뷰\u0026#34;); } else if result[0] == 1 { println!(\u0026#34;긍정적인 리뷰\u0026#34;); } else { println!(\u0026#34;알 수 없음\u0026#34;); } } Ok(()) } fn main() -\u0026gt; Result\u0026lt;()\u0026gt; { let device = Device::cuda_if_available(0)?; let vs = unsafe { nn::VarBuilder::from_mmaped_safetensors(\u0026amp;[\u0026#34;nsmc.safetensors\u0026#34;], DType::F32, \u0026amp;device)? }; let mut model = TextCLassifier::new(vs)?; let tokenizer = Tokenizer::from_file(\u0026#34;tokenizer.json\u0026#34;).map_err(anyhow::Error::msg)?; model.interaction(tokenizer, \u0026amp;device)?;\\ Ok(()) } 메인 함수에서 safetensors 포맷으로 저장된 모델 파일과 토크나이저를 가져온 후, 생성한 모델의 interaction 함수를 사용하면 터미널과 상호작용 할 수 있다.\n끝 실제 사용 예시\n영화 리뷰를 입력하세요: 와 진짜 재밌는 영화임미다!! 긍정적인 리뷰 영화 리뷰를 입력하세요: 이딴걸 영화라고 처 만들었냐? 부정적인 리뷰 영화 리뷰를 입력하세요: 감독 나와라 ㅋㅋ 부정적인 리뷰 영화 리뷰를 입력하세요: 이 정도면 괜찮은 영화 아닌가? 긍정적인 리뷰 영화 리뷰를 입력하세요: 감동적이네료 ㅠㅜㅜ 긍정적인 리뷰 영화 리뷰를 입력하세요: 볼 가치도 없는듯 ㅋ 부정적인 리뷰 무려 5MB 정도의 작은 모델인데, 직접 영화 리뷰를 써보니 정확도가 90% 이상 나온다고 느꼈다. 거대 언어 모델에 비하면 리뷰를 긍정 또는 부정으로 분류하는 작업밖에 할 수 없어 초라해 보이지만 속도가 중요한 실제 서비스, 또는 임베디드 분야에서 유용할 것 같다는 생각이 들었다.\n사실 성공할 줄은 몰랐는데, 직접 모델을 구현하며 여러 시행착오를 겪었던 점에서 의미가 있는 것 같다.\n","permalink":"https://blog.hee.blue/posts/ai/sentiment-ai-implementation-with-rust/","summary":"개요 파이썬은 내가 제일 많이 사용하는 언어 중 하나이다. 또한 파이썬은 PyTorch, Tensorflow, Keras 등 굉장히 편리하고 사용하기 쉬운 라이브러리가 많다. 하지만 대부분의 파이썬 머신러닝 라이브러리의 중심 부분은 파이썬으로 구현되지 않고 C, C++ 등의 저수준 언어로 구현되어있다. 파이썬은 인공신경망 등의 알고리즘을 실행시키기에는 너무나도 느리기 때문이다.\nPyTorch는 인공지능 개발에 필수적인 라이브러리라고 할 수 있다. 파이토치에서 제공하는 수백, 혹은 수천 개의 머신러닝을 위한 API는 코드 몇 줄로 모델을 만들거나, 학습시키고 추론하기 위한 다양한 함수들을 제공한다.","title":"Rust로 감성분석 AI 구현하기 (LSTM, GRU)"},{"content":"개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.\n학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.1 충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러) 초연한 자세 어떠한 오류가 터져도 화내지 않는 강건한 정신 채팅 데이터셋 AI Hub에서 가져온 주제별 텍스트 일상 대화 데이터 데이터 전처리 라이브러리(pypi) unsloth accelerate transformers trl datasets peft etc. 채팅 데이터셋 전처리 JSON으로 구성된 채팅 데이터셋은 대충 전처리하면 다음과 같은 텍스트 형태가 된다.\nuser: 점심 메뉴 정하신 분 bot: 누룽지 끓여 먹을까 고민 user: 나 아침 든든하게 먹으니 힘 난다 user: 아 나 점심 뭐 먹지 bot: 점심은 사모님이 주심 후후 user: 나 점심 내장국밥 픽 user: **가 오늘 만들어야 함? bot: 누룽지는 간식으로 먹어야징 user: 돼지국밥으로 바꿀까 user: 누룽지 맛있겠어... 나두 bot: 여기 먹을 거 개많음 ㄷㄷ user: 아니면 두루치기 먹을가 user: 국밥은 언제나 맛있음 bot: 떡볶이 이런 거 먹을 수 있음 키키 user: 먹는 게 제이 좋아! user: 국밥 1그릇 배달 안 해주겠지 ㅠ bot: 수제비랑 밀키트 다 나왕 user: 떡볶이 파는 데가 없음 여기 \u0026lt;|endoftext|\u0026gt; 여기서 문제가 하나 발생한다. 우리는 로봇과 1대1로 대화하기를 원하는데, 이 데이터셋은 화자가 최대 3명이기 때문에 1번 화자는 user, 2번 화자는 bot, 3번 화자는 다시 user로 치환해버려서 user는 이중인격자가 되는 것을 확인할 수 있다. 이 부분은 학습이 완료된 후 알아채서 다음에는 user1, user2, user3, ...이런 식으로 시도해보려고 한다.\n또한 이름도 개인정보 보호 차원에서 별 모양 기호(asterisk, *)로 뜨는데, 적당하게 \u0026lt;name\u0026gt;으로 바꿨으면 더 좋았을 것 같다.\n마지막으로 오타가 상당히 많은데, 오타 좀 내면 더욱 사람과 비슷해지지 않을까 하는 생각이 들어 오타 교정은 수행하지 않았다. 사실 몇만 줄의 텍스트 파일을 맞춤법 검사기를 돌려 일일이 수정하는 것도 미친 짓이라고 생각했다.\n학습 코드 import torch print(torch.__version__) from tqdm.auto import tqdm from unsloth import FastLanguageModel from trl import SFTTrainer from datasets import load_dataset from transformers import TrainingArguments 먼저 필요한 모듈을 import한다. 여기서 unsloth 모듈은 뭐하는 놈일까?\nUnsloth는 무엇인가 unsloth는 여러 언어모델들을 파인튜닝 시 2배~5배 빠르게, 메모리 사용량을 80%까지 줄일 수 있다고 주장하는 라이브러리다. 실제로 vram 사용량을 꽤 줄였던 것으로 기억한다.\n1 A100 40GB 🤗Hugging Face Flash Attention 🦥Unsloth Open Source 🦥Unsloth Pro Alpaca 1x 1.04x 1.98x 15.64x LAION Chip2 1x 0.92x 1.61x 20.73x OASST 1x 1.19x 2.17x 14.83x Slim Orca 1x 1.18x 2.22x 14.82x 위 표는 unsloth의 깃허브 readme에서 가져온 벤치마크 결과이다. 최대 2배 빠르다는 건 이해할 수 있지만, 아무리 봐도 14~15배 빨라진다는 unsloth pro는 그짓말 같긴 하다.\n아무튼 unsloth는 파이토치 버전, 쿠다 버전, GPU Compute Capability에 따라 설치 명령어가 약간씩 다르니 꼭 README를 참고하여 설치하자.\n모델 \u0026amp; 토크나이저 다운로드 model, tokenizer = FastLanguageModel.from_pretrained(\u0026#39;beomi/Llama-3-Open-Ko-8B\u0026#39;, max_seq_length=1024, dtype=None, load_in_4bit=True) unsloth에서 제공하는 FastLanguageModel을 사용하여 transformers 라이브러리와 비슷한 방식으로 모델을 다운로드 받을 수 있다.\n그리고 load_in_4bit 인자를 사용하여 모델을 4비트 양자화한 상태로 불러온다. 양자화를 하지 않으면 나중에 학습할 때 VRAM 점유율이 4090의 24GB를 초과하여 OOM(CUDA Out Of Memory) 오류가 터진다. 다만 양자화로 인해 모델의 성능 저하도 있는 것 같다. max_seq_length는 입력 토큰의 최대 길이이다. 1024 토큰이 좀 작을 수도 있지만, OOM이 무서워서 일단 1024로 했다. 나중에 RoPE Scaling이라는 기술을 활용하면 모델의 context size를 파인튜닝 없이 늘릴 수 있다고 한다.\nPEFT 모델로 변환 model = FastLanguageModel.get_peft_model( model, r=16, target_modules = [\u0026#34;q_proj\u0026#34;, \u0026#34;k_proj\u0026#34;, \u0026#34;v_proj\u0026#34;, \u0026#34;o_proj\u0026#34;, \u0026#34;gate_proj\u0026#34;, \u0026#34;up_proj\u0026#34;, \u0026#34;down_proj\u0026#34;,], lora_alpha=32, lora_dropout=0, bias=\u0026#34;none\u0026#34;, use_gradient_checkpointing=\u0026#34;unsloth\u0026#34;, max_seq_length=1024, use_rslora=False, loftq_config=None, ) PEFT(Parameter Efficient Fine-Tuning)는 Huggingface 커뮤니티에서 개발 중인 라이브러리다. PEFT는 모델의 전체 파라미터를 학습하는 대신, LoRA와 같은 기술을 사용하여 필요한 작은 수의 파라미터만 학습해서 컴퓨팅 비용을 줄일 수 있는 모듈이다. 기존 학습 코드를 조금만 수정하면 효율적으로 학습이 가능하다는 점이 장점이다.\nUnsloth는 PEFT 모듈 + $\\alpha$를 활용해서 속도와 메모리 사용량을 줄이는 방식이기 때문에 두 라이브러리는 다르다.\nUnsloth에서 제공하는 FastLanguageModel에서 모델을 PEFT 모델로 변환할 수 있다. 내부적으로 LoRA를 사용하는 듯하다. LoRA 기술에 대해서는 기본적인 동작 원리만 이해하고 자세히는 알지 못해서 나중에 따로 공부해보고 싶다.\n데이터셋 불러오기 dataset = load_dataset(\u0026#39;json\u0026#39;, data_files=\u0026#39;chat.json\u0026#39;, split=\u0026#39;train\u0026#39;) huggingface datasets 모듈으로 아까 전처리한 텍스트 파일을 대화 주제 단위로 자르고, 다음과 같은 JSON 형식으로 변환해서 load_dataset 함수로 불러왔다.\n[ { \u0026#34;text\u0026#34;: \u0026#34;user: 점심 메뉴 정하신 분 bot: 누룽지 끓여 먹을까 고민...[이하생략]\u0026gt;\u0026#34; } ] Trainer 구현 trainer = SFTTrainer( model=model, train_dataset=dataset, dataset_text_field=\u0026#39;text\u0026#39;, max_seq_length=1024, tokenizer=tokenizer, args=TrainingArguments( per_device_train_batch_size=2, gradient_accumulation_steps=4, warmup_steps=10, bf16=True, logging_steps=10, output_dir=\u0026#39;results\u0026#39;, optim=\u0026#39;adamw_8bit\u0026#39;, ), ) trl에서 제공하는 SFTTrainer(Supervised Fine-Tuning Trainer)를 사용하면 쉽게 파인튜닝할 수 있다. 여기서 중요한 코드가 TrainingArguments 부분이라고 생각하는데, 학습에 필요한 learning rate 등을 TrainingArguments에 인자로 넘기는 부분이기 때문이다. per_device_train_batch_size 대신 auto_find_batch_size=True를 쓰면 자동으로 최적의 batch size를 찾아준다. 사실 TrainingArguments는 unsloth의 readme를 귀찮아서 대충 그냥 갖다 쓴거라 뭔지는 나도 잘 모른다.\n학습 이제 학습은 굉장히 간단해진다.\ntrainer.train() 끝.\nUnsloth가 편리한게 다음과 같이 코드를 입력하면 자동으로 llama.cpp에서 돌아가는 GGUF 파일 형식으로 변환하고 양자화까지 해준다는 점이다.\nmodel.save_pretrained_gguf(\u0026#34;dir\u0026#34;, tokenizer, quantization_method = \u0026#34;q4_k_m\u0026#34;) model.save_pretrained_gguf(\u0026#34;dir\u0026#34;, tokenizer, quantization_method = \u0026#34;q8_0\u0026#34;) model.save_pretrained_gguf(\u0026#34;dir\u0026#34;, tokenizer, quantization_method = \u0026#34;f16\u0026#34;) 학습 결과 이제 챗봇과 대화를 해보자.\n대화 기록\nuser: 안녕 bot: 안녕 user: 오늘 날씨 좋다 ㅎㅎ bot: 오늘 날씨가 너무 좋아 키키 user: 밖에 나가자 bot: 그러게 완전 가을이야~ user: 여름인데? bot: 여름이 아니지 않나? user: 여름맞는데? bot: 아직 여름인 거 같은데? user: 지금 내 말이 틀렸다고 주장하는 거니? bot: 그렇다고 할 수는 없잖아~ 나도 좋아 키키 내가 파인튜닝을 너무 대충 해서 그런지는 모르겠는데, 대화를 하면 할 수록 배가 산으로 가는 현상을 볼 수 있다.\n아니면 약 4MB 용량의 적은 데이터셋으로 학습을 진행해서 성능이 구린 이유도 있는 것 같다.\n마치며 대략 2년 전부터 진짜 사람같은 챗봇을 만들고 싶어서 여러가지 시도를 해보고 있다. ChatGPT 등 성능이 좋은 모델의 api를 사용하면 되지 않느냐고 생각할 수 있지만, API를 사용하는 방식은 내 성미에 맞지 않기도 하고 gpt 특유의 어색한 말투가 너무 부자연스러워서 대화에 몰입할 수 없었다.\n재미있는 건 1년 전에 EleutherAI의 polyglot-ko라는 pretrained 모델을 사용해서 파인튜닝한 결과물이 지금보다 좋았다고 느꼈다는 사실이다. 아무래도 polyglot-ko는 완전히 처음부터 한국어 데이터로 pretraining한 반면, 라마와 같은 모델은 영어의 비중이 높고, 한국어가 거의 없는 데이터셋으로 학습 후 누군가 한국어로 다시 파인튜닝했기 때문에 한국어 능력이 부족하지 않았나 추측해본다.\n라마와 같은 pretrained 모델 하나를 만드려면 엄청난 비용과 시간이 든다. 그럼에도 불구하고 2022년에 공개된 polyglot-ko 이후 처음부터 한국어로 학습한 모델이 전무하다고 할 수 있을 정도로 현재 한국어 LLM 생태계는 기존 영어 모델을 파인튜닝하고, 다시 파인튜닝하는 부분에 초점이 맞춰져있는 것 같다. 그래서 나는 네이버 등의 회사가 비교적 작은 모델인 한국어 sLLM을 공개했으면 좋겠다는 바람이 있다.\n개인적인 의견이므로 부정확할 수 있으니 참고하지 마시오.\n","permalink":"https://blog.hee.blue/posts/ai/unsloth-llama3-fine-tuning/","summary":"개요 글을 작성하는 현 시점 기준으로 약 2주 전 라마3가 발표되었다. 성능이 어떨지 궁금했기 때문에 이번 기회에 라마3을 파인튜닝하자고 결심했다. 허깅페이스에 누군가 한국어 데이터셋에 맞춰 학습시켜놓은 모델이 있었기 때문에 파인튜닝 시 기본 영어 모델보다 한국어 성능이 나을 것이라고 판단하여 이 모델을 베이스로 사용하기로 했다.\n학습 전 준비물 개발 환경 (GPU 클라우드) RTX 4090 X 1 Ubuntu 22.04 PyTorch 2.2.0 CUDA 12.1 충분한 자본금(4090 기준 시간 당 약 0.3~0.4달러) 초연한 자세 어떠한 오류가 터져도 화내지 않는 강건한 정신 채팅 데이터셋 AI Hub에서 가져온 주제별 텍스트 일상 대화 데이터 데이터 전처리 라이브러리(pypi) unsloth accelerate transformers trl datasets peft etc.","title":"Efficient LLaMA3 Fine-Tuning with Unsloth"},{"content":"개요 BLE 라이브러리에서 소개했던 라이브러리 중, react-native-ble-plx를 어떻게 사용하는지에 대해 기록을 남긴다.\n패키지 설치 yarn add react-native-ble-plx yarn 또는 npm을 사용하여 먼저 설치를 진행한다. 만약 npx expo install react-native-ble-plx 명령어를 사용하여 설치를 진행한다면, app.json 또는 app.config.js의 플러그인 부분에 라이브러리가 자동으로 추가되는 듯하다.\nyarn expo prebuild expo의 prebuild 명령어를 이용해 네이티브 파일을 생성한다.\n{ \u0026#34;expo\u0026#34;: { \u0026#34;plugins\u0026#34;: [\u0026#34;react-native-ble-plx\u0026#34;] } } 최상위 폴더의 app.json에 라이브러리를 추가하거나, 추가가 되어있는지 확인한다.\nyarn android 안드로이드용으로 빌드 시 위 명령어를 입력한다. ios 빌드는 맥이 없어서 실행을 못 해봤다. ㅠ\nBLE 라이브러리 정리에서도 설명했지만, neverForLocation 권한 플래그가 라이브러리 자체에서 포함됐기 때문에 AndroidManifest.xml 파일의 BLUETOOTH_SCAN 부분에 다음과 같이 tools:remove 속성을 추가해야 한다.\n\u0026lt;uses-permission android:name=\u0026#34;android.permission.BLUETOOTH_SCAN\u0026#34; tools:remove=\u0026#34;android:usesPermissionFlags\u0026#34;/\u0026gt; 그리고 AndroidManifest.xml 최상단 \u0026lt;manifest\u0026gt; 태그에 다음과 같이 xmlns를 추가해줘야 빌드 시 오류가 나지 않는다.\n\u0026lt;manifest xmlns:android=\u0026#34;http://schemas.android.com/apk/res/android\u0026#34; xmlns:tools=\u0026#34;http://schemas.android.com/tools\u0026#34;\u0026gt; 구현 코드 인스턴스 생성 다음과 같이 먼저 BleManager 인스턴스를 생성한다.\nimport { BleManager } from \u0026#39;react-native-ble-plx\u0026#39; export const manager = new BleManager() 반드시 하나의 인스턴스만 허용된다고 한다. 전역변수로 선언하면 되는 듯하다.\n인스턴스를 삭제하려면 manager.destroy() 함수를 활용하면 된다.\n권한 부여 (Android) import { PermissionsAndroid, Platform, ToastAndroid } from \u0026#34;react-native\u0026#34;; import { BleManager, Device, ScanMode } from \u0026#34;react-native-ble-plx\u0026#34;; const requestBluetoothPermission = async () =\u0026gt; { if (Platform.OS === \u0026#34;ios\u0026#34;) { return true; // iOS는 권한 부여 코드가 따로 필요하지 않음 } if ( Platform.OS === \u0026#34;android\u0026#34; \u0026amp;\u0026amp; PermissionsAndroid.PERMISSIONS.ACCESS_FINE_LOCATION ) { const apiLevel = parseInt(Platform.Version.toString(), 10); if (apiLevel \u0026lt; 31) { const granted = await PermissionsAndroid.request( PermissionsAndroid.PERMISSIONS.ACCESS_FINE_LOCATION ); return granted === PermissionsAndroid.RESULTS.GRANTED; } if ( PermissionsAndroid.PERMISSIONS.BLUETOOTH_SCAN \u0026amp;\u0026amp; PermissionsAndroid.PERMISSIONS.BLUETOOTH_CONNECT ) { const result = await PermissionsAndroid.requestMultiple([ PermissionsAndroid.PERMISSIONS.BLUETOOTH_SCAN, PermissionsAndroid.PERMISSIONS.BLUETOOTH_CONNECT, PermissionsAndroid.PERMISSIONS.ACCESS_FINE_LOCATION, ]); return ( result[\u0026#34;android.permission.BLUETOOTH_CONNECT\u0026#34;] === PermissionsAndroid.RESULTS.GRANTED \u0026amp;\u0026amp; result[\u0026#34;android.permission.BLUETOOTH_SCAN\u0026#34;] === PermissionsAndroid.RESULTS.GRANTED \u0026amp;\u0026amp; result[\u0026#34;android.permission.ACCESS_FINE_LOCATION\u0026#34;] === PermissionsAndroid.RESULTS.GRANTED ); // 모든 권한이 허용되었으면 true를 반환 } ToastAndroid.show(\u0026#34;권한을 모두 허용해주세요.\u0026#34;, 3000); // 권한 허용이 안 되어있으면 toast 메시지 출력 return false; } }; iOS는 따로 권한 요청 코드가 필요 없다.\n기기 검색 권한 요청이 성공하면 디바이스를 검색할 수 있다. 다음은 검색된 모든 기기의 이름을 출력하는 코드이다.\nbleManager.startDeviceScan(null, { scanMode: ScanMode.Balanced }, (error, device) =\u0026gt; { if (error) { console.log(error); return error; } if (!device) return; console.log(device.name); }) 아까 생성한 bleManager 인스턴스의 startDeviceScan 함수로 스캔을 시작한다.\n첫 번째 인자에 uuid를 입력하면 검색되는 기기를 해당 uuid에 대해 필터링할 수 있다. null을 입력하면 따로 필터링 과정을 거치지 않고 모든 기기가 검색된다.\n인자로 넘긴 콜백 함수는 어떤 디바이스를 찾았을 때 호출된다.\n끝 네이티브(Kotlin, Swift)에 비해 RN은 블루투스 등을 제어하기가 까다로운데, 이 라이브러리는 iOS와 안드로이드를 동시에 지원해서 생산성이 뛰어나다.\n또한 코드 몇 줄로 백그라운드에서 실행이 가능한 것으로 보인다.\n","permalink":"https://blog.hee.blue/posts/react-native/react-native-ble-plx-usage/","summary":"개요 BLE 라이브러리에서 소개했던 라이브러리 중, react-native-ble-plx를 어떻게 사용하는지에 대해 기록을 남긴다.\n패키지 설치 yarn add react-native-ble-plx yarn 또는 npm을 사용하여 먼저 설치를 진행한다. 만약 npx expo install react-native-ble-plx 명령어를 사용하여 설치를 진행한다면, app.json 또는 app.config.js의 플러그인 부분에 라이브러리가 자동으로 추가되는 듯하다.\nyarn expo prebuild expo의 prebuild 명령어를 이용해 네이티브 파일을 생성한다.\n{ \u0026#34;expo\u0026#34;: { \u0026#34;plugins\u0026#34;: [\u0026#34;react-native-ble-plx\u0026#34;] } } 최상위 폴더의 app.json에 라이브러리를 추가하거나, 추가가 되어있는지 확인한다.\nyarn android 안드로이드용으로 빌드 시 위 명령어를 입력한다.","title":"react-native-ble-plx 사용 방법"},{"content":"개요 몇 달 전에 산 ESP32를 쓸 곳이 별로 없어서 계속 방치하던 중 재실 감지 센서 아이디어가 떠올랐다.\n원래 esp32를 재실감지용으로 사용하는 사람이 많아 ESPresense라는 굉장히 편리한 프로젝트가 있다. 이거 말고도 ESPHome이라는 프로젝트가 있는데, ESPHome은 좀 무거운 듯하고 와이파이 연결 에러가 자꾸 발생해서 그냥 ESPresense를 사용하기로 했다.\nESPresense 설치 설치 방법은 굉장히 간단하다. 그냥 ESPresense 공식 사이트에 접속하면 웹페이지 내에서 크롬 시리얼 포트를 통해 esp32에 프로그램을 설치할 수 있다. 설치 후 와이파이 세팅까지 끝내면 내부망으로 esp32 웹서버에 접속할 수 있다.\n문제 발생 생각해보니 esp32는 ble를 사용하여 스마트폰을 식별한다. 그런데 안드로이드는 아이폰과 다르게 ble로 식별할 수 없다. 따라서 별도의 앱을 설치해야 하는데, 이것때문에 빡쳐서 때려침\n","permalink":"https://blog.hee.blue/posts/embedded/esphome-presence/","summary":"개요 몇 달 전에 산 ESP32를 쓸 곳이 별로 없어서 계속 방치하던 중 재실 감지 센서 아이디어가 떠올랐다.\n원래 esp32를 재실감지용으로 사용하는 사람이 많아 ESPresense라는 굉장히 편리한 프로젝트가 있다. 이거 말고도 ESPHome이라는 프로젝트가 있는데, ESPHome은 좀 무거운 듯하고 와이파이 연결 에러가 자꾸 발생해서 그냥 ESPresense를 사용하기로 했다.\nESPresense 설치 설치 방법은 굉장히 간단하다. 그냥 ESPresense 공식 사이트에 접속하면 웹페이지 내에서 크롬 시리얼 포트를 통해 esp32에 프로그램을 설치할 수 있다. 설치 후 와이파이 세팅까지 끝내면 내부망으로 esp32 웹서버에 접속할 수 있다.","title":"ESP32를 이용한 재실감지 센서 만들기"},{"content":"리액트 네이티브 BLE(Bluetooth Low Energy) 라이브러리가 여러 개가 있는데, 그 중 사용해본 라이브러리를 설명하겠다. 아래는 깃허브 스타 수를 기준으로 정렬하였다.\nreact-native-ble-plx 이 라이브러리는 크로스 플랫폼(Android, iOS)을 지원하여 개발자가 블루투스 서비스를 사용하기 쉽게 만들어 놓은 라이브러리다. 현재 깃허브 스타 수가 제일 많고, 기여가 활발한 편이다.\nexpo를 지원해서, expo install 커맨드로 설치하면 AndroidManifest.xml이나 build.gradle 등의 네이티브 파일을 수정하지 않아도 된다.\n다만 사용 중 모든 BLE 디바이스가 검색이 안되는 문제가 발생하여 대략 3일동안 삽질했는데, 아래 이슈를 보고 해결하였다.\n#1014\n안드로이드 13 이상에서는 neverForLocation(링크) 권한 플래그가 새로 생겼는데, 이 권한 플래를 android:usesPermissionFlags에 포함하면 일부 BLE 비콘이 검색 결과에서 필터링된다. 나는 android/app/src/AndroidManifest.xml 파일에 직접 플래그를 포함하지 않았지만, 라이브러리 자체에서 플래그를 포함했기 때문에 비콘이 검색 결과에 뜨지 않았던 것이다. 따라서 AndroidManifest.xml 파일의 권한 부분에 다음과 같이 tools:remove 속성을 추가해야 한다.\n\u0026lt;uses-permission android:name=\u0026#34;android.permission.BLUETOOTH_SCAN\u0026#34; tools:remove=\u0026#34;android:usesPermissionFlags\u0026#34; /\u0026gt; react-native-ble-manager react-native-ble-plx와 비슷하지만, BLE 스캔 방식이 더 불편하다고 느꼈다. 예를 들어 디바이스 스캔 후 결과값을 출력하려면 RN의 네이티브 모듈에 이벤트 리스너를 추가해야 한다.\nreact-native-beacons-manager 기여가 거의 없는 수준이라 추천하지는 않는다.\n","permalink":"https://blog.hee.blue/posts/react-native/ble/","summary":"리액트 네이티브 BLE(Bluetooth Low Energy) 라이브러리가 여러 개가 있는데, 그 중 사용해본 라이브러리를 설명하겠다. 아래는 깃허브 스타 수를 기준으로 정렬하였다.\nreact-native-ble-plx 이 라이브러리는 크로스 플랫폼(Android, iOS)을 지원하여 개발자가 블루투스 서비스를 사용하기 쉽게 만들어 놓은 라이브러리다. 현재 깃허브 스타 수가 제일 많고, 기여가 활발한 편이다.\nexpo를 지원해서, expo install 커맨드로 설치하면 AndroidManifest.xml이나 build.gradle 등의 네이티브 파일을 수정하지 않아도 된다.\n다만 사용 중 모든 BLE 디바이스가 검색이 안되는 문제가 발생하여 대략 3일동안 삽질했는데, 아래 이슈를 보고 해결하였다.","title":"React Native BLE 라이브러리 정리"}]